{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just to know how much time will it take to run this entire ipython notebook \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from surprise import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading data from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58698779</th>\n",
       "      <td>10774</td>\n",
       "      <td>510180</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96212476</th>\n",
       "      <td>17064</td>\n",
       "      <td>510180</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6901473</th>\n",
       "      <td>1367</td>\n",
       "      <td>510180</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49973109</th>\n",
       "      <td>9003</td>\n",
       "      <td>510180</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20393918</th>\n",
       "      <td>3870</td>\n",
       "      <td>510180</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie    user  rating       date\n",
       "58698779  10774  510180     3.0 1999-11-11\n",
       "96212476  17064  510180     2.0 1999-11-11\n",
       "6901473    1367  510180     5.0 1999-11-11\n",
       "49973109   9003  510180     3.0 1999-11-11\n",
       "20393918   3870  510180     2.0 1999-11-11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv', sep=',', \n",
    "                       names=['movie', 'user','rating','date'])\n",
    "df.date = pd.to_datetime(df.date)\n",
    "\n",
    "# we are arranging the ratings according to time.\n",
    "df.sort_values(by='date', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there any entry that has Nan values...??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95880696</th>\n",
       "      <td>16992</td>\n",
       "      <td>962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95963294</th>\n",
       "      <td>17002</td>\n",
       "      <td>51082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie   user  rating date\n",
       "95880696  16992    962     NaN  NaT\n",
       "95963294  17002  51082     NaN  NaT"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it returns True for a row that has atleast one Nan value.\n",
    "null_bool = df.isnull().any(axis = 1)\n",
    "df[null_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[null_bool].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Nan values in our dataframe :  0\n"
     ]
    }
   ],
   "source": [
    "print(\"No of Nan values in our dataframe : \", sum(df.isnull().any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to find the duplicates if any..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1211 duplicate rating entries in the data..\n"
     ]
    }
   ],
   "source": [
    "dup_bool = df.duplicated(['movie','user','rating'])\n",
    "dups = sum(dup_bool) # by considering all columns..( including timestamp )\n",
    "print(\"There are {} duplicate rating entries in the data..\".format(dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16969    762\n",
       "16968    449\n",
       "Name: movie, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's find what are the movies that has duplicate entries of user ratings....\n",
    "df[dup_bool].movie.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Removing those duplicate entries__  (inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[dup_bool].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of duplicate rows (movie, user, rating) entries : 0\n"
     ]
    }
   ],
   "source": [
    "print(\"No of duplicate rows (movie, user, rating) entries :\", sum(df.duplicated()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No of ratings that we have, after removing nan rows and duplicates are : 100479045\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNo of ratings that we have, after removing nan rows and duplicates are :\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No. of Ratings, Users and Movies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data \n",
      "--------------------------------------------------\n",
      "\n",
      "Total no of ratings : 100479045\n",
      "Total No of Users   : 480189\n",
      "Total No of movies  : 17770\n"
     ]
    }
   ],
   "source": [
    "movies = df.movie.value_counts()\n",
    "users = df.user.value_counts()\n",
    "ratings = df.rating\n",
    "print(\"Total data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal no of ratings :\",df.shape[0])\n",
    "print(\"Total No of Users   :\", len(users))\n",
    "print(\"Total No of movies  :\", len(movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_train_df = df.iloc[:int(df.shape[0]*0.80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80383236, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58698779</th>\n",
       "      <td>10774</td>\n",
       "      <td>510180</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96212476</th>\n",
       "      <td>17064</td>\n",
       "      <td>510180</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie    user  rating       date\n",
       "58698779  10774  510180     3.0 1999-11-11\n",
       "96212476  17064  510180     2.0 1999-11-11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data \n",
      "--------------------------------------------------\n",
      "\n",
      "Total no of ratings : 80383236\n",
      "Total No of Users   : 405024\n",
      "Total No of movies  : 17423\n"
     ]
    }
   ],
   "source": [
    "movies = big_train_df.movie.value_counts()\n",
    "users = big_train_df.user.value_counts()\n",
    "print(\"Training data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal no of ratings :\",big_train_df.shape[0])\n",
    "print(\"Total No of Users   :\", len(users))\n",
    "print(\"Total No of movies  :\", len(movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating sparse matrix for Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy import sparse\n",
    "start = datetime.now()\n",
    "if os.path.isfile('sample/large/train_sparse_matrix.npz'):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    train_sparse_matrix = sparse.load_npz('sample/large/train_sparse_matrix.npz')\n",
    "    print(\"DONE..\")\n",
    "else: \n",
    "    print(\"We are creating sparse_matrix from the dataframe..\")\n",
    "    # create sparse_matrix and store it for after usage.\n",
    "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
    "    # It should be in such a way that, MATRIX[row, col] = data\n",
    "    train_sparse_matrix = sparse.csr_matrix((big_train_df.rating.values, (big_train_df.user.values,\n",
    "                                               big_train_df.movie.values)))\n",
    "    \n",
    "    print('Done. It\\'s shape is : (user, movie) : ',train_sparse_matrix.shape)\n",
    "    print('Saving it into disk for furthur usage..')\n",
    "    # save it into disk\n",
    "    sparse.save_npz(\"sample/large/train_sparse_matrix.npz\", train_sparse_matrix)\n",
    "    print('Done..\\n')\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,m = train_sparse_matrix.shape\n",
    "elem = train_sparse_matrix.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Of matrix : 99.82927583214679 % \n"
     ]
    }
   ],
   "source": [
    "print(\"Sparsity Of matrix : {} % \".format(  (1-(elem/(u*m))) * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_test_df = df.iloc[int(df.shape[0]*0.80) : ]\n",
    "big_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52863848</th>\n",
       "      <td>9617</td>\n",
       "      <td>316390</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2005-08-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12989568</th>\n",
       "      <td>2462</td>\n",
       "      <td>605375</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-08-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie    user  rating       date\n",
       "52863848   9617  316390     2.0 2005-08-08\n",
       "12989568   2462  605375     4.0 2005-08-08"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data \n",
      "--------------------------------------------------\n",
      "\n",
      "Total no of ratings : 20095809\n",
      "Total No of Users   : 349327\n",
      "Total No of movies  : 17757\n"
     ]
    }
   ],
   "source": [
    "movies = big_test_df.movie.value_counts()\n",
    "users = big_test_df.user.value_counts()\n",
    "\n",
    "print(\"Test data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal no of ratings :\",big_test_df.shape[0])\n",
    "print(\"Total No of Users   :\", len(users))\n",
    "print(\"Total No of movies  :\", len(movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating sparse matrix for Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in your pwd, getting it from disk....\n",
      "DONE..\n",
      "0:00:00.855393\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy import sparse\n",
    "start = datetime.now()\n",
    "if os.path.isfile('sample/large/test_sparse_matrix.npz'):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    test_sparse_matrix = sparse.load_npz('sample/test_sparse_matrix.npz')\n",
    "    print(\"DONE..\")\n",
    "else: \n",
    "    print(\"We are creating sparse_matrix from the dataframe..\")\n",
    "    # create sparse_matrix and store it for after usage.\n",
    "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
    "    # It should be in such a way that, MATRIX[row, col] = data\n",
    "    test_sparse_matrix = sparse.csr_matrix((big_test_df.rating.values, (big_test_df.user.values,\n",
    "                                               big_test_df.movie.values)))\n",
    "    \n",
    "    print('Done. It\\'s shape is : (user, movie) : ',test_sparse_matrix.shape)\n",
    "    print('Saving it into disk for furthur usage..')\n",
    "    # save it into disk\n",
    "    sparse.save_npz(\"sample/test_sparse_matrix.npz\", test_sparse_matrix)\n",
    "    print('Done..\\n')\n",
    " \n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,m = test_sparse_matrix.shape\n",
    "elem = test_sparse_matrix.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20095713"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Of matrix : 99.95731855608713 % \n"
     ]
    }
   ],
   "source": [
    "print(\"Sparsity Of matrix : {} % \".format(  (1-(elem/(u*m))) * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Average Ratings (from Train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean matrix of ratings ( whether a user rated that movie or not)\n",
    "is_rated = train_sparse_matrix!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': 3.5829128738184792}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the global average of ratings in our train set.\n",
    "global_average = train_sparse_matrix.sum()/train_sparse_matrix.count_nonzero()\n",
    "averages['global'] = global_average\n",
    "averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.182377049180328"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the user averages in dictionary (key: userid, value: avg rating)\n",
    "#_____________________________________________________________________#\n",
    "\n",
    "# \".A1\" is for converting Column_Matrix to 1-D numpy array \n",
    "sum_of_ratings_per_user = train_sparse_matrix.sum(axis=1).A1\n",
    "# no of ratings that each user has given.\n",
    "no_of_ratings_per_user = is_rated.sum(axis=1).A1\n",
    "\n",
    "# creae a dictonary of users and their average ratigns..\n",
    "average_user_ratings = { i : sum_of_ratings_per_user[i]/no_of_ratings_per_user[i]  \n",
    "                                 for i in range(train_sparse_matrix.shape[0]) \n",
    "                                    if no_of_ratings_per_user[i] !=0}\n",
    "\n",
    "# add user averages to th eaverages dictionary\n",
    "averages['user'] = average_user_ratings\n",
    "\n",
    "# test it..\n",
    "averages['user'][97]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7808264297834113"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the Movie Average ratings in dictionary (key: movieId, value: avg_rating)\n",
    "#_____________________________________________________________________#\n",
    "\n",
    "# sum of the ratings that a movie got by any user(who rated that movie..)\n",
    "sum_of_ratings_per_movie = train_sparse_matrix.sum(axis=0).A1\n",
    "# no of ratings that a movie got.\n",
    "no_of_ratings_per_movie = is_rated.sum(axis=0).A1\n",
    "\n",
    "average_movie_ratings = {i : sum_of_ratings_per_movie[i]/ no_of_ratings_per_movie[i] \n",
    "                                for i in range(train_sparse_matrix.shape[1])\n",
    "                                    if no_of_ratings_per_movie[i]!=0 }\n",
    "\n",
    "# add thie'per_movie' avg ratings to averages dictionary\n",
    "averages['movie'] = average_movie_ratings\n",
    "\n",
    "# test this dictionary\n",
    "averages['movie'][30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Trainset and testset for Surprise based alorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading it from the disk\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('sample/large/trainset.pickle'):\n",
    "    print('loading it from the disk')\n",
    "    trainset = pickle.load(open('sample/large/trainset.pickle', 'rb'))\n",
    "    print('done')\n",
    "else:\n",
    "    print('creating it from sparse_matrix ( if it is loaded)')\n",
    "    \n",
    "    train_users, train_movies, train_ratings = sparse.find(train_sparse_matrix)\n",
    "    \n",
    "   \n",
    "\n",
    "    print('preparing train dataframe with users, movies and ratings of the trainset..')\n",
    "    surp_train = pd.DataFrame({'user': train_users,\n",
    "                           'movie': train_movies,\n",
    "                           'rating': train_ratings}, )\n",
    "    surp_train = surp_train[['user','movie','rating']]\n",
    "    print(surp_train.head(2))\n",
    "    \n",
    "    print('Creating trainset from the dataframe...')\n",
    "    trainset = Dataset.load_from_df(surp_train,Reader(rating_scale=(1,5))).build_full_trainset()\n",
    "    \n",
    "    print('No of unique users, unique movies and ratings in train data', end=' : ')\n",
    "    print('(users, movies, ratings) : ({}, {}, {})'.format(trainset.n_users, \n",
    "                                                           trainset.n_items,\n",
    "                                                           trainset.n_ratings))\n",
    "    \n",
    "    # saving testset to dsik\n",
    "    print('saving it to disk..')\n",
    "    start = datetime.now()\n",
    "    with open('sample/large/trainset.pickle', 'wb') as f:\n",
    "        pickle.dump(trainset, f)\n",
    "    print(datetime.now() - start)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405024, 17423, 80382095)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.n_users, trainset.n_items, trainset.n_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testset from the disk..\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('sample/large/testset.pickle'):\n",
    "    print('loading testset from the disk..')\n",
    "    testset = pickle.load(open('sample/large/testset.pickle', 'rb'))\n",
    "    print('Done')\n",
    "else:\n",
    "    print('creating tesetset from sparse matrix.( if it is loaded )')\n",
    "    \n",
    "    test_users, test_movies, test_ratings = sparse.find(test_sparse_matrix)\n",
    "    \n",
    "    print('No of unique users, unique movies..', end=' : ')\n",
    "    print(len(np.unique(test_users)),  len(np.unique(test_movies)))\n",
    "\n",
    "    print(\"No of ratings in test set :\",len(test_ratings))\n",
    "\n",
    "    testset = list(zip(test_users, test_movies, test_ratings))\n",
    "\n",
    "    # saving testset to dsik\n",
    "    print('saving it to disk..')\n",
    "    start = datetime.now()\n",
    "    with open('sample/large/testset.pickle', 'wb') as f:\n",
    "        pickle.dump(testset, f)\n",
    "    print(datetime.now() - start)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic function to run any surprise based algorithm \n",
    "\n",
    "    - given prefectly initialized ALGO, TRAINSET and TESTSET\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# get  (actual_list , predicted_list) ratings given list \n",
    "# of predictions (prediction is a class in Surprise).    \n",
    "##########################################################\n",
    "def get_ratings(predictions, return_actual=False):\n",
    "    if return_actual:\n",
    "        actual = np.array([pred.r_ui for pred in predictions])\n",
    "        \n",
    "    pred = np.array([pred.est for pred in predictions])\n",
    "    \n",
    "    if return_actual:\n",
    "        return actual, pred\n",
    "    \n",
    "    return pred\n",
    "\n",
    "################################################################\n",
    "# get ''rmse'' and ''mape'' , given list of prediction classes \n",
    "################################################################\n",
    "def get_errors(predictions, print_them=False):\n",
    "\n",
    "    actual, pred = get_ratings(predictions, return_actual=True)\n",
    "    rmse = np.sqrt(np.mean((pred - actual)**2))\n",
    "    mape = np.mean(np.abs(pred - actual)/actual)\n",
    "\n",
    "    return rmse, mape*100\n",
    "\n",
    "##################################################################################\n",
    "# It will return predicted ratings, rmse and mape of both train and test data   #\n",
    "##################################################################################\n",
    "def run_surprise(algo, trainset, testset,evaluate_train=False, verbose=True): \n",
    "    '''\n",
    "        return train_dict, test_dict\n",
    "    \n",
    "        It returns two dictionaries, one for train and the other is for test\n",
    "        Each of them have 3 key-value pairs, which specify ''rmse'', ''mape'', and ''predicted ratings''.\n",
    "    '''\n",
    "    start = datetime.now()\n",
    "    # dictionaries that stores metrics for train and test..\n",
    "    train = dict()\n",
    "    test = dict()\n",
    "    \n",
    "    # train the algorithm with the trainset\n",
    "    st = datetime.now()\n",
    "    print('Training the model...')\n",
    "    algo.fit(trainset)\n",
    "    print('Done. time taken : {} \\n'.format(datetime.now()-st))\n",
    "    \n",
    "    # ---------------- Evaluating train data--------------------#\n",
    "    if evaluate_train:\n",
    "        st = datetime.now()\n",
    "        print('Evaluating the model with train data..')\n",
    "        # get the train predictions (list of prediction class inside Surprise)\n",
    "        train_preds = algo.test(trainset.build_testset())\n",
    "        # get predicted ratings from the train predictions..\n",
    "        train_actual_ratings, train_pred_ratings = get_ratings(train_preds)\n",
    "        # get ''rmse'' and ''mape'' from the train predictions.\n",
    "        train_rmse, train_mape = get_errors(train_preds)\n",
    "        print('time taken : {}'.format(datetime.now()-st))\n",
    "        if verbose:\n",
    "            print('-'*15)\n",
    "            print('Train Data')\n",
    "            print('-'*15)\n",
    "            print(\"RMSE : {}\\n\\nMAPE : {}\\n\".format(train_rmse, train_mape))\n",
    "    \n",
    "        #store them in the train dictionary\n",
    "        if verbose:\n",
    "            print('adding train results in the dictionary..')\n",
    "        train['rmse'] = train_rmse\n",
    "        train['mape'] = train_mape\n",
    "        train['predictions'] = train_pred_ratings\n",
    "    else:\n",
    "        print('\\n we are skipping model evaluation with train data..\\n')\n",
    "        train = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    #------------ Evaluating Test data---------------#\n",
    "    st = datetime.now()\n",
    "    print('\\nEvaluating for test data...')\n",
    "    # get the predictions( list of prediction classes) of test data\n",
    "    test_preds = algo.test(testset)\n",
    "    # get the predicted ratings from the list of predictions\n",
    "    test_pred_ratings = get_ratings(test_preds, return_actual=False)\n",
    "    # get error metrics from the predicted and actual ratings\n",
    "    test_rmse, test_mape = get_errors(test_preds)\n",
    "    print('time taken : {}'.format(datetime.now()-st))\n",
    "    \n",
    "    if verbose:\n",
    "        print('-'*15)\n",
    "        print('Test Data')\n",
    "        print('-'*15)\n",
    "        print(\"RMSE : {}\\n\\nMAPE : {}\\n\".format(test_rmse, test_mape))\n",
    "    # store them in test dictionary\n",
    "    if verbose:\n",
    "        print('storing the test results in test dictionary...')\n",
    "    test['rmse'] = test_rmse\n",
    "    test['mape'] = test_mape\n",
    "    test['predictions'] = test_pred_ratings\n",
    "    \n",
    "    print('\\n'+'-'*45)\n",
    "    print('Total time taken to run this algorithm :', datetime.now() - start)\n",
    "    \n",
    "    # return two dictionaries train and test\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Global dictionary that stores rmse and mape for all the models....\n",
    "\n",
    "- It stores the metrics in a dictionary of dictionaries\n",
    "\n",
    "> __keys__ : model names(string)\n",
    "\n",
    "> __value__: dict(__key__ : metric, __value__ : value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_evaluation_train = dict()\n",
    "models_evaluation_test = dict()\n",
    "\n",
    "models_evaluation_train, models_evaluation_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline Model  ( with User and Item biases )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import BaselineOnly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Predicted_rating : ( baseline prediction )\n",
    "\n",
    ">$   \\large {\\hat{r}_{ui} = b_{ui} =\\mu + b_u + b_i} $\n",
    "\n",
    "- ####  Optimization function ( Least Squares Problem )\n",
    "\n",
    "> $ \\large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - (\\mu + b_u + b_i)\\right)^2 +\n",
    "\\lambda \\left(b_u^2 + b_i^2 \\right).\\text {        [mimimize } {b_u, b_i]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is pretrained and saved in the directory spedified....\n",
      "There is no need to retrain the model. Just get it from the disk...\n"
     ]
    }
   ],
   "source": [
    "from surprise import dump\n",
    "\n",
    "if os.path.isfile('sample/large/bsl_algo'):\n",
    "    \n",
    "    print('This model is pretrained and saved in the directory spedified....')\n",
    "    print('There is no need to retrain the model. Just get it from the disk...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting it from the disk...(both predicted_ratings and algorithm...)\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "## get the predicted ratings and the algorithm itself from the disk\n",
    "print('Getting it from the disk...(both predicted_ratings and algorithm...)')\n",
    "test_pred_ratings , bsl_algo = dump.load('sample/large/bsl_algo')\n",
    "print('Done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bsl_algo': {'mape': 30.90283402482485, 'rmse': 1.0052001851833883}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sample/large/metrics_bsl_algo','rb') as f:\n",
    "    models_evaluation_test['bsl_algo'] = pickle.load(f)\n",
    "\n",
    "models_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "#####################################################################################################\n",
    "# print('We have to train the model from test dataset. It may take a while..')\n",
    "    \n",
    "# # specifying which method to use (ALS or SGD) to calculate biases\n",
    "# bsl_options = {'method': 'sgd',\n",
    "#            'learning_rate': .001\n",
    "#            }\n",
    "# bsl = BaselineOnly(bsl_options=bsl_options)\n",
    "\n",
    "\n",
    "# bsl_train_results, bsl_test_results = run_surprise(bsl, trainset, testset,\n",
    "#                                                evaluate_train=False ,verbose=True)\n",
    "\n",
    "# # store them in models dictionary..\n",
    "# models_evaluation_train['bsl'] = bsl_train_results\n",
    "# models_evaluation_test['bsl'] = bsl_test_results\n",
    "\n",
    "# # store the trained model in disk...\n",
    "# dump.dump('sample/large/bsl_algo', predictions=models_evaluation_test['bsl']['predictions'],\n",
    "#           algo=bsl_algo, verbose=1)\n",
    "\n",
    "# # saving test results in dictionary in python\n",
    "# models_evaluation_test['bsl_algo'] = {'rmse':bsl_test_results['rmse'], 'mape':bsl_test_results['mape']}\n",
    "\n",
    "# with open('sample/large/metrics_bsl_algo', mode='wb') as f:\n",
    "#     pickle.dump(models_evaluation_test['bsl_algo'], f)\n",
    "#####################################################################################################\n",
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. KNN with Baseline_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBaseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __predicted Rating__ : ( ___ based on User-User similarity ___ )\n",
    "\n",
    "\\begin{align} \\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
    "\\text{sim}(u, v) \\cdot (r_{vi} - b_{vi})} {\\sum\\limits_{v \\in\n",
    "N^k_i(u)} \\text{sim}(u, v)} \\end{align}\n",
    "\n",
    "- $\\pmb{b_{ui}}$ -  _Baseline prediction_ of (user,movie) rating\n",
    "\n",
    "\n",
    "- $ \\pmb {N_i^k (u)}$ - Set of __K similar__ users (neighbours) of __user (u)__ who rated __movie(i)__  \n",
    "\n",
    "\n",
    "- _sim (u, v)_ - __Similarity__ between users __u and v__  \n",
    "    - Generally, it will be cosine similarity or Pearson correlation coefficient. \n",
    "    - But we use __shrunk Pearson-baseline correlation coefficient__, which is based on the pearsonBaseline similarity ( we take base line predictions instead of mean rating of user/item)\n",
    "        - Computation of the correlation coefficient is based only on the common user support.\n",
    "        - similarities based on a greater user support are more reliable ie., Users who has more number of common movie ratings are considered as more similar than users who has few no of movies  in common which are rated.\n",
    "        - \\begin{align}\\begin{aligned}\\text{pearson_baseline_shrunk_sim}(u, v) &= \\frac{|I_{uv}| - 1}{|I_{uv}| - 1 + \\text{shrinkage}} \\cdot \\hat{\\rho}_{uv}\\end{aligned}\\end{align}\n",
    "            - $\\pmb{|I_{uv}|}$ - No of common movies between users(u and v)\n",
    "            - __shrinkage__ - kind of hyperparameter. The defalut value suggested is ___100___\n",
    "                - __0__ : There is no shrinkage at all ( It is normal pearson correlation coefficient ) \n",
    "            - $ \\pmb {\\hat \\rho_uv}$ - Pearson Correlation Coefficient ( between users )\n",
    "                - \\begin{align} \\text{pearson_baseline_sim}(u, v) = \\hat{\\rho}_{uv} = \\frac{\n",
    "    \\sum\\limits_{i \\in I_{uv}} (r_{ui} -  b_{ui}) \\cdot (r_{vi} -\n",
    "    b_{vi})} {\\sqrt{\\sum\\limits_{i \\in I_{uv}} (r_{ui} -  b_{ui})^2}\n",
    "    \\cdot \\sqrt{\\sum\\limits_{i \\in I_{uv}} (r_{vi} -  b_{vi})^2}} \\end{align}\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ------------ or \n",
    " --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ Predicted rating __ ( based on Item Item similarity ):\n",
    " \\begin{align} \\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{j \\in N^k_u(i)}\\text{sim}(i, j) \\cdot (r_{uj} - b_{uj})} {\\sum\\limits_{j \\in N^k_u(j)} \\text{sim}(i, j)} \\end{align}\n",
    "\n",
    "    -  ___Notations follows same as above (user user based predicted rating ) ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can do both and blend them ( see if we can better results when combined ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## We are not doing Knn_baseline with USER_USER similarities is.., It will take up more ram and memory\n",
    "- ## Instead We will just perform knn based on the ITEM_ITEM similarities.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 KNN with User User similarities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - - We are not doing knn with user user similarities., because we have around 480k users and we will run out of memory while computing similarities.., and Even if we can somehow make the RAM available, It will take like forever...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we specify , how to compute similarities and what to consider with sim_options to our algorithm\n",
    "# sim_options = {'user_based' : True,\n",
    "#                'name': 'pearson_baseline',\n",
    "#                'shrinkage': 100,\n",
    "#                'min_support': 2\n",
    "#               } \n",
    "\n",
    "# bsl_options = {'method': 'sgd'} # we keep other parameters like regularization parameter and learning_rate as default values.\n",
    "\n",
    "# knn_bsl_u = KNNBaseline(k=40, sim_options = sim_options, bsl_options = bsl_options)\n",
    "\n",
    "# knn_bsl_u_train_results, knn_bsl_u_test_results = run_surprise(knn_bsl_u, trainset, testset, \n",
    "#                                                                evaluate_train=False, verbose=True)\n",
    "\n",
    "# models_evaluation_train['knn_bsl_u'] = knn_bsl_u_train_results\n",
    "# models_evaluation_test['knn_bsl_u']  = knn_bsl_u_test_results\n",
    "\n",
    "# models_evaluation_test['knn_bsl_u'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 KNN with Item Item similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is pretrained and saved in the directory spedified....\n",
      "There is no need to retrain the model. Just get it from the disk...\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('sample/large/knn_bsl_m'):\n",
    "    \n",
    "    print('This model is pretrained and saved in the directory spedified....')\n",
    "    print('There is no need to retrain the model. Just get it from the disk...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting it from the disk...(both predicted_ratings and algorithm...)\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "## get the predicted ratings and the algorithm itself from the disk\n",
    "print('Getting it from the disk...(both predicted_ratings and algorithm...)')\n",
    "knn_bsl_m_test_pred_ratings , knn_bsl_m = dump.load('sample/large/knn_bsl_m')\n",
    "print('Done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bsl_algo': {'mape': 30.90283402482485, 'rmse': 1.0052001851833883},\n",
       " 'knn_bsl_m': {'mape': 30.14567877982246, 'rmse': 0.9984568872852136}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sample/large/metrics_knn_bsl_m','rb') as f:\n",
    "    models_evaluation_test['knn_bsl_m'] = pickle.load(f)\n",
    "\n",
    "models_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Estimating biases using sgd...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Done. time taken : 1:15:13.788940 \n",
      "\n",
      "\n",
      " we are skipping model evaluation with train data..\n",
      "\n",
      "\n",
      "Evaluating for test data...\n",
      "time taken : 1:08:19.145859\n",
      "---------------\n",
      "Test Data\n",
      "---------------\n",
      "RMSE : 0.9984568872852136\n",
      "\n",
      "MAPE : 30.14567877982246\n",
      "\n",
      "storing the test results in test dictionary...\n",
      "\n",
      "---------------------------------------------\n",
      "Total time taken to run this algorithm : 2:23:32.935913\n",
      "Storing the trained model (with predicted ratings) in the disk..\n",
      "The dump has been saved as file sample/large/knn_bsl_m\n",
      "Done..\n"
     ]
    }
   ],
   "source": [
    "# # we specify , how to compute similarities and what to consider with sim_options to our algorithm\n",
    "# sim_options = {'user_based' : False,\n",
    "#                'name': 'pearson_baseline',\n",
    "#                'shrinkage': 100,\n",
    "#                'min_support': 2\n",
    "#               } \n",
    "# # we keep other parameters like regularization parameter and learning_rate as default values.\n",
    "# bsl_options = {'method': 'sgd'} \n",
    "\n",
    "# knn_bsl_m = KNNBaseline(k=40, sim_options = sim_options, bsl_options = bsl_options)\n",
    "\n",
    "# knn_bsl_m_train_results, knn_bsl_m_test_results = run_surprise(knn_bsl_m, trainset, testset, verbose=True,\n",
    "#                                                                evaluate_train=False)\n",
    "\n",
    "# models_evaluation_train['knn_bsl_m'] = knn_bsl_m_train_results\n",
    "# models_evaluation_test['knn_bsl_m']  = knn_bsl_m_test_results\n",
    "\n",
    "# # store the trained model in disk...\n",
    "# print('Storing the trained model (with predicted ratings) in the disk..')\n",
    "# dump.dump('sample/large/knn_bsl_m', predictions=models_evaluation_test['knn_bsl_m']['predictions'],\n",
    "#           algo=knn_bsl_m, verbose=1)\n",
    "# print('Done..')\n",
    "# # saving test results in dictionary in python\n",
    "# models_evaluation_test['knn_bsl_m'] = {'rmse':knn_bsl_m_test_results['rmse'],\n",
    "#                                        'mape':knn_bsl_m_test_results['mape']}\n",
    "\n",
    "# with open('sample/large/metrics_knn_bsl_m', mode='wb') as f:\n",
    "#     pickle.dump(models_evaluation_test['knn_bsl_m'], f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Matrix Factorization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 SVD -  MF algorithm with user item interactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ Predicted Rating : __\n",
    "    - $ \\large \\hat r_{ui} = \\mu + b_u + b_i + q_i^Tp_u $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Optimization problem with user item interactions and regularization (to avoid overfitting)__\n",
    "\n",
    "\n",
    "- $\\Large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
    "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is pretrained and saved in the directory spedified....\n",
      "There is no need to retrain the model. Just get it from the disk...\n"
     ]
    }
   ],
   "source": [
    "from surprise import dump\n",
    "\n",
    "if os.path.isfile('sample/large/svd'):\n",
    "    \n",
    "    print('This model is pretrained and saved in the directory spedified....')\n",
    "    print('There is no need to retrain the model. Just get it from the disk...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting svd from the disk...(both predicted_ratings and algorithm...)\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "## get the predicted ratings and the algorithm itself from the disk\n",
    "print('Getting svd from the disk...(both predicted_ratings and algorithm...)')\n",
    "svd_test_pred_ratings , svd = dump.load('sample/large/svd')\n",
    "print('Done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bsl_algo': {'mape': 30.90283402482485, 'rmse': 1.0052001851833883},\n",
       " 'knn_bsl_m': {'mape': 30.14567877982246, 'rmse': 0.9984568872852136},\n",
       " 'svd': {'mape': 29.491243711503127, 'rmse': 0.9868320443529861}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sample/large/metrics_svd','rb') as f:\n",
    "    models_evaluation_test['svd'] = pickle.load(f)\n",
    "\n",
    "models_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing the trained svd along with the predictions..\n",
      "The dump has been saved as file sample/large/svd\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "####################################################################################################\n",
    "# print('We have to train the model from test dataset. It may take a while..(90 min)')\n",
    "\n",
    "# svd = SVD(n_factors=100, n_epochs=20, random_state=15, verbose=True)\n",
    "\n",
    "\n",
    "# svd_train_results, svd_test_results = run_surprise(svd, trainset, testset,\n",
    "#                                                    evaluate_train=False, verbose=True)\n",
    "\n",
    "# # store them in models dictionary..\n",
    "# models_evaluation_train['svd'] = svd_train_results\n",
    "# models_evaluation_test['svd'] = svd_test_results\n",
    "\n",
    "# store the trained svd model in disk...(along with the predicted ratings...)\n",
    "# print('storing the trained svd along with the predictions..')\n",
    "# dump.dump('sample/large/svd', predictions=models_evaluation_test['svd']['predictions'],\n",
    "#           algo=svd, verbose=1)\n",
    "\n",
    "# # saving test results in dictionary in python\n",
    "# models_evaluation_test['svd'] = {'rmse':svd_test_results['rmse'],\n",
    "#                                  'mape':svd_test_results['mape']}\n",
    "\n",
    "# with open('sample/large/metrics_svd', mode='wb') as f:\n",
    "#     pickle.dump(models_evaluation_test['svd'], f)\n",
    "####################################################################################################\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.2 SVD -  with Implicit feedback of Items(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from surprise import SVDpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ Predicted Rating : __ \n",
    "    - $ \\large \\hat{r}_{ui} = \\mu + b_u + b_i + q_i^T\\left(p_u +\n",
    "    |I_u|^{-\\frac{1}{2}} \\sum_{j \\in I_u}y_j\\right) $ \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - $ \\pmb{I_u}$ --- the set of all items rated by user u\n",
    "\n",
    "- $\\pmb{y_j}$ --- Our new set of item factors that capture implicit ratings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Optimization problem with user item interactions and regularization (to avoid overfitting)__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "- $ \\Large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
    "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2 + y_j^2\\right) $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      " processing epoch 0\n",
      " processing epoch 1\n",
      " processing epoch 2\n"
     ]
    }
   ],
   "source": [
    "# svdpp = SVDpp(n_factors=20, random_state=15, verbose=True)\n",
    "# svdpp_train_results, svdpp_test_results = run_surprise(svdpp, trainset, testset)\n",
    "\n",
    "# models_evaluation_train['svdpp'] = svdpp_train_results\n",
    "# models_evaluation_test['svdpp'] = svdpp_test_results\n",
    "\n",
    "# models_evaluation_test['svdpp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data metrics of models that we have trained so far "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bsl_algo</th>\n",
       "      <th>knn_bsl_m</th>\n",
       "      <th>svd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mape</th>\n",
       "      <td>30.902834</td>\n",
       "      <td>30.145679</td>\n",
       "      <td>29.491244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.005200</td>\n",
       "      <td>0.998457</td>\n",
       "      <td>0.986832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bsl_algo  knn_bsl_m        svd\n",
       "mape  30.902834  30.145679  29.491244\n",
       "rmse   1.005200   0.998457   0.986832"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(models_evaluation_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
