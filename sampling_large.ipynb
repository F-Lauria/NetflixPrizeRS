{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just to know how much time will it take to run this entire ipython notebook \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from surprise import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating sparse matrix for Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in your pwd, getting it from disk....\n",
      "DONE..\n",
      "0:00:04.600554\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy import sparse\n",
    "start = datetime.now()\n",
    "if os.path.isfile('sample/large/train_sparse_matrix.npz'):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    train_sparse_matrix = sparse.load_npz('sample/large/train_sparse_matrix.npz')\n",
    "    print(\"DONE..\")\n",
    "else: \n",
    "    print(\"There is no Train_Sparse Matrix. Get it first..\")\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train data\n",
      "---------------\n",
      "No of elements : 80384405\n",
      "No of users    : 405041\n",
      "No of Movies   : 17424\n"
     ]
    }
   ],
   "source": [
    "u_trn, m_trn, r_trn = sparse.find(train_sparse_matrix)\n",
    "\n",
    "print('\\nTrain data')\n",
    "print('-'*15)\n",
    "\n",
    "print('No of elements :', len(r_trn))\n",
    "print('No of users    :', len(np.unique(u_trn)))\n",
    "print('No of Movies   :', len(np.unique(m_trn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Of matrix : 99.8292709259195 % \n"
     ]
    }
   ],
   "source": [
    "r, c = train_sparse_matrix.shape\n",
    "elem = train_sparse_matrix.count_nonzero()\n",
    "print(\"Sparsity Of matrix : {} % \".format(  (1-(elem/(r*c))) * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating sparse matrix for Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in your pwd, getting it from disk....\n",
      "DONE..\n",
      "0:00:01.052938\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy import sparse\n",
    "start = datetime.now()\n",
    "if os.path.isfile('sample/large/test_sparse_matrix.npz'):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    test_sparse_matrix = sparse.load_npz('sample/large/test_sparse_matrix.npz')\n",
    "    print(\"DONE..\")\n",
    "else: \n",
    "    print(\"There is no Train_Sparse Matrix. Get it first..\")\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test data\n",
      "---------------\n",
      "No of elements : 20096102\n",
      "No of users    : 349312\n",
      "No of Movies   : 17757\n"
     ]
    }
   ],
   "source": [
    "u_tst, m_tst, r_tst = sparse.find(test_sparse_matrix)\n",
    "\n",
    "print('\\nTest data')\n",
    "print('-'*15)\n",
    "\n",
    "print('No of elements :', len(r_tst))\n",
    "print('No of users    :', len(np.unique(u_tst)))\n",
    "print('No of Movies   :', len(np.unique(m_tst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Of matrix : 99.95731855608713 % \n"
     ]
    }
   ],
   "source": [
    "r,c = test_sparse_matrix.shape\n",
    "elem = test_sparse_matrix.count_nonzero()\n",
    "\n",
    "print(\"Sparsity Of matrix : {} % \".format(  (1-(elem/(u*m))) * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing Trainset and testset for Surprise based alorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading it from the disk\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('sample/large/trainset.pickle'):\n",
    "    print('loading it from the disk')\n",
    "    trainset = pickle.load(open('sample/large/trainset.pickle', 'rb'))\n",
    "    print('done')\n",
    "else:\n",
    "    print('creating it from sparse_matrix ( if it is loaded)')\n",
    "    \n",
    "    train_users, train_movies, train_ratings = sparse.find(train_sparse_matrix)\n",
    "    \n",
    "   \n",
    "\n",
    "    print('preparing train dataframe with users, movies and ratings of the trainset..')\n",
    "    surp_train = pd.DataFrame({'user': train_users,\n",
    "                           'movie': train_movies,\n",
    "                           'rating': train_ratings}, )\n",
    "    surp_train = surp_train[['user','movie','rating']]\n",
    "    print(surp_train.head(2))\n",
    "    \n",
    "    print('Creating trainset from the dataframe...')\n",
    "    trainset = Dataset.load_from_df(surp_train,Reader(rating_scale=(1,5))).build_full_trainset()\n",
    "    \n",
    "    print('No of unique users, unique movies and ratings in train data', end=' : ')\n",
    "    print('(users, movies, ratings) : ({}, {}, {})'.format(trainset.n_users, \n",
    "                                                           trainset.n_items,\n",
    "                                                           trainset.n_ratings))\n",
    "    \n",
    "    # saving testset to dsik\n",
    "    print('saving it to disk..')\n",
    "    start = datetime.now()\n",
    "    with open('sample/large/trainset.pickle', 'wb') as f:\n",
    "        pickle.dump(trainset, f)\n",
    "    print(datetime.now() - start)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405024, 17423, 80382095)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.n_users, trainset.n_items, trainset.n_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testset from the disk..\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('sample/large/testset.pickle'):\n",
    "    print('loading testset from the disk..')\n",
    "    testset = pickle.load(open('sample/large/testset.pickle', 'rb'))\n",
    "    print('Done')\n",
    "else:\n",
    "    print('creating tesetset from sparse matrix.( if it is loaded )')\n",
    "    \n",
    "    test_users, test_movies, test_ratings = sparse.find(test_sparse_matrix)\n",
    "    \n",
    "    print('No of unique users, unique movies..', end=' : ')\n",
    "    print(len(np.unique(test_users)),  len(np.unique(test_movies)))\n",
    "\n",
    "    print(\"No of ratings in test set :\",len(test_ratings))\n",
    "\n",
    "    testset = list(zip(test_users, test_movies, test_ratings))\n",
    "\n",
    "    # saving testset to dsik\n",
    "    print('saving it to disk..')\n",
    "    start = datetime.now()\n",
    "    with open('sample/large/testset.pickle', 'wb') as f:\n",
    "        pickle.dump(testset, f)\n",
    "    print(datetime.now() - start)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic function to run any surprise based algorithm \n",
    "\n",
    "    - given prefectly initialized ALGO, TRAINSET and TESTSET\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# get  (actual_list , predicted_list) ratings given list \n",
    "# of predictions (prediction is a class in Surprise).    \n",
    "##########################################################\n",
    "def get_ratings(predictions, return_actual=False):\n",
    "    if return_actual:\n",
    "        actual = np.array([pred.r_ui for pred in predictions])\n",
    "        \n",
    "    pred = np.array([pred.est for pred in predictions])\n",
    "    \n",
    "    if return_actual:\n",
    "        return actual, pred\n",
    "    \n",
    "    return pred\n",
    "\n",
    "################################################################\n",
    "# get ''rmse'' and ''mape'' , given list of prediction classes \n",
    "################################################################\n",
    "def get_errors(predictions, print_them=False):\n",
    "\n",
    "    actual, pred = get_ratings(predictions, return_actual=True)\n",
    "    rmse = np.sqrt(np.mean((pred - actual)**2))\n",
    "    mape = np.mean(np.abs(pred - actual)/actual)\n",
    "\n",
    "    return rmse, mape*100\n",
    "\n",
    "##################################################################################\n",
    "# It will return predicted ratings, rmse and mape of both train and test data   #\n",
    "##################################################################################\n",
    "def run_surprise(algo, trainset, testset,evaluate_train=False, verbose=True): \n",
    "    '''\n",
    "        return train_dict, test_dict\n",
    "    \n",
    "        It returns two dictionaries, one for train and the other is for test\n",
    "        Each of them have 3 key-value pairs, which specify ''rmse'', ''mape'', and ''predicted ratings''.\n",
    "    '''\n",
    "    start = datetime.now()\n",
    "    # dictionaries that stores metrics for train and test..\n",
    "    train = dict()\n",
    "    test = dict()\n",
    "    \n",
    "    # train the algorithm with the trainset\n",
    "    st = datetime.now()\n",
    "    print('Training the model...')\n",
    "    algo.fit(trainset)\n",
    "    print('Done. time taken : {} \\n'.format(datetime.now()-st))\n",
    "    \n",
    "    # ---------------- Evaluating train data--------------------#\n",
    "    if evaluate_train:\n",
    "        st = datetime.now()\n",
    "        print('Evaluating the model with train data..')\n",
    "        # get the train predictions (list of prediction class inside Surprise)\n",
    "        train_preds = algo.test(trainset.build_testset())\n",
    "        # get predicted ratings from the train predictions..\n",
    "        train_actual_ratings, train_pred_ratings = get_ratings(train_preds)\n",
    "        # get ''rmse'' and ''mape'' from the train predictions.\n",
    "        train_rmse, train_mape = get_errors(train_preds)\n",
    "        print('time taken : {}'.format(datetime.now()-st))\n",
    "        if verbose:\n",
    "            print('-'*15)\n",
    "            print('Train Data')\n",
    "            print('-'*15)\n",
    "            print(\"RMSE : {}\\n\\nMAPE : {}\\n\".format(train_rmse, train_mape))\n",
    "    \n",
    "        #store them in the train dictionary\n",
    "        if verbose:\n",
    "            print('adding train results in the dictionary..')\n",
    "        train['rmse'] = train_rmse\n",
    "        train['mape'] = train_mape\n",
    "        train['predictions'] = train_pred_ratings\n",
    "    else:\n",
    "        print('\\n we are skipping model evaluation with train data..\\n')\n",
    "        train = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    #------------ Evaluating Test data---------------#\n",
    "    st = datetime.now()\n",
    "    print('\\nEvaluating for test data...')\n",
    "    # get the predictions( list of prediction classes) of test data\n",
    "    test_preds = algo.test(testset)\n",
    "    # get the predicted ratings from the list of predictions\n",
    "    test_pred_ratings = get_ratings(test_preds, return_actual=False)\n",
    "    # get error metrics from the predicted and actual ratings\n",
    "    test_rmse, test_mape = get_errors(test_preds)\n",
    "    print('time taken : {}'.format(datetime.now()-st))\n",
    "    \n",
    "    if verbose:\n",
    "        print('-'*15)\n",
    "        print('Test Data')\n",
    "        print('-'*15)\n",
    "        print(\"RMSE : {}\\n\\nMAPE : {}\\n\".format(test_rmse, test_mape))\n",
    "    # store them in test dictionary\n",
    "    if verbose:\n",
    "        print('storing the test results in test dictionary...')\n",
    "    test['rmse'] = test_rmse\n",
    "    test['mape'] = test_mape\n",
    "    test['predictions'] = test_pred_ratings\n",
    "    \n",
    "    print('\\n'+'-'*45)\n",
    "    print('Total time taken to run this algorithm :', datetime.now() - start)\n",
    "    \n",
    "    # return two dictionaries train and test\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Global dictionary that stores rmse and mape for all the models....\n",
    "\n",
    "- It stores the metrics in a dictionary of dictionaries\n",
    "\n",
    "> __keys__ : model names(string)\n",
    "\n",
    "> __value__: dict(__key__ : metric, __value__ : value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_evaluation_train = dict()\n",
    "models_evaluation_test = dict()\n",
    "\n",
    "models_evaluation_train, models_evaluation_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline Model  ( with User and Item biases )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import BaselineOnly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Predicted_rating : ( baseline prediction )\n",
    "\n",
    ">$   \\large {\\hat{r}_{ui} = b_{ui} =\\mu + b_u + b_i} $\n",
    "\n",
    "- ####  Optimization function ( Least Squares Problem )\n",
    "\n",
    "> $ \\large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - (\\mu + b_u + b_i)\\right)^2 +\n",
    "\\lambda \\left(b_u^2 + b_i^2 \\right).\\text {        [mimimize } {b_u, b_i]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is pretrained and saved in the directory spedified....\n",
      "Getting it from the disk...(both predicted_ratings and algorithm...)\n",
      "Done...\n",
      "Getting rmse and mape values from disk..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bsl_algo': {'mape': 30.90283402482485, 'rmse': 1.0052001851833883}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import dump\n",
    "\n",
    "if os.path.isfile('sample/large/bsl_algo'):\n",
    "    \n",
    "    print('This model is pretrained and saved in the directory spedified....')\n",
    "    \n",
    "    ## get the predicted ratings and the algorithm itself from the disk\n",
    "    print('Getting it from the disk...(both predicted_ratings and algorithm...)')\n",
    "    test_pred_ratings , bsl_algo = dump.load('sample/large/bsl_algo')\n",
    "    print('Done...')\n",
    "    \n",
    "    print('Getting rmse and mape values from disk..')\n",
    "    with open('sample/large/metrics_bsl_algo','rb') as f:\n",
    "        models_evaluation_test['bsl_algo'] = pickle.load(f)\n",
    "\n",
    "models_evaluation_test      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "#####################################################################################################\n",
    "# print('We have to train the model from test dataset. It may take a while..')\n",
    "    \n",
    "# # specifying which method to use (ALS or SGD) to calculate biases\n",
    "# bsl_options = {'method': 'sgd',\n",
    "#            'learning_rate': .001\n",
    "#            }\n",
    "# bsl = BaselineOnly(bsl_options=bsl_options)\n",
    "\n",
    "\n",
    "# bsl_train_results, bsl_test_results = run_surprise(bsl, trainset, testset,\n",
    "#                                                evaluate_train=False ,verbose=True)\n",
    "\n",
    "# # store them in models dictionary..\n",
    "# models_evaluation_train['bsl'] = bsl_train_results\n",
    "# models_evaluation_test['bsl'] = bsl_test_results\n",
    "\n",
    "# # store the trained model in disk...\n",
    "# dump.dump('sample/large/bsl_algo', predictions=models_evaluation_test['bsl']['predictions'],\n",
    "#           algo=bsl_algo, verbose=1)\n",
    "\n",
    "# # saving test results in dictionary in python\n",
    "# models_evaluation_test['bsl_algo'] = {'rmse':bsl_test_results['rmse'], 'mape':bsl_test_results['mape']}\n",
    "\n",
    "# with open('sample/large/metrics_bsl_algo', mode='wb') as f:\n",
    "#     pickle.dump(models_evaluation_test['bsl_algo'], f)\n",
    "#####################################################################################################\n",
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. KNN with Baseline_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBaseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __predicted Rating__ : ( ___ based on User-User similarity ___ )\n",
    "\n",
    "\\begin{align} \\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
    "\\text{sim}(u, v) \\cdot (r_{vi} - b_{vi})} {\\sum\\limits_{v \\in\n",
    "N^k_i(u)} \\text{sim}(u, v)} \\end{align}\n",
    "\n",
    "- $\\pmb{b_{ui}}$ -  _Baseline prediction_ of (user,movie) rating\n",
    "\n",
    "\n",
    "- $ \\pmb {N_i^k (u)}$ - Set of __K similar__ users (neighbours) of __user (u)__ who rated __movie(i)__  \n",
    "\n",
    "\n",
    "- _sim (u, v)_ - __Similarity__ between users __u and v__  \n",
    "    - Generally, it will be cosine similarity or Pearson correlation coefficient. \n",
    "    - But we use __shrunk Pearson-baseline correlation coefficient__, which is based on the pearsonBaseline similarity ( we take base line predictions instead of mean rating of user/item)\n",
    "        - Computation of the correlation coefficient is based only on the common user support.\n",
    "        - similarities based on a greater user support are more reliable ie., Users who has more number of common movie ratings are considered as more similar than users who has few no of movies  in common which are rated.\n",
    "        - \\begin{align}\\begin{aligned}\\text{pearson_baseline_shrunk_sim}(u, v) &= \\frac{|I_{uv}| - 1}{|I_{uv}| - 1 + \\text{shrinkage}} \\cdot \\hat{\\rho}_{uv}\\end{aligned}\\end{align}\n",
    "            - $\\pmb{|I_{uv}|}$ - No of common movies between users(u and v)\n",
    "            - __shrinkage__ - kind of hyperparameter. The defalut value suggested is ___100___\n",
    "                - __0__ : There is no shrinkage at all ( It is normal pearson correlation coefficient ) \n",
    "            - $ \\pmb {\\hat \\rho_uv}$ - Pearson Correlation Coefficient ( between users )\n",
    "                - \\begin{align} \\text{pearson_baseline_sim}(u, v) = \\hat{\\rho}_{uv} = \\frac{\n",
    "    \\sum\\limits_{i \\in I_{uv}} (r_{ui} -  b_{ui}) \\cdot (r_{vi} -\n",
    "    b_{vi})} {\\sqrt{\\sum\\limits_{i \\in I_{uv}} (r_{ui} -  b_{ui})^2}\n",
    "    \\cdot \\sqrt{\\sum\\limits_{i \\in I_{uv}} (r_{vi} -  b_{vi})^2}} \\end{align}\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ------------ or \n",
    " --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ Predicted rating __ ( based on Item Item similarity ):\n",
    " \\begin{align} \\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{j \\in N^k_u(i)}\\text{sim}(i, j) \\cdot (r_{uj} - b_{uj})} {\\sum\\limits_{j \\in N^k_u(j)} \\text{sim}(i, j)} \\end{align}\n",
    "\n",
    "    -  ___Notations follows same as above (user user based predicted rating ) ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can do both and blend them ( see if we can better results when combined ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## We are not doing Knn_baseline with USER_USER similarities is.., It will take up more ram and memory\n",
    "- ## Instead We will just perform knn based on the ITEM_ITEM similarities.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 KNN with User User similarities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - - We are not doing knn with user user similarities., because we have around 480k users and we will run out of memory while computing similarities.., and Even if we can somehow make the RAM available, It will take like forever...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we specify , how to compute similarities and what to consider with sim_options to our algorithm\n",
    "# sim_options = {'user_based' : True,\n",
    "#                'name': 'pearson_baseline',\n",
    "#                'shrinkage': 100,\n",
    "#                'min_support': 2\n",
    "#               } \n",
    "\n",
    "# bsl_options = {'method': 'sgd'} # we keep other parameters like regularization parameter and learning_rate as default values.\n",
    "\n",
    "# knn_bsl_u = KNNBaseline(k=40, sim_options = sim_options, bsl_options = bsl_options)\n",
    "\n",
    "# knn_bsl_u_train_results, knn_bsl_u_test_results = run_surprise(knn_bsl_u, trainset, testset, \n",
    "#                                                                evaluate_train=False, verbose=True)\n",
    "\n",
    "# models_evaluation_train['knn_bsl_u'] = knn_bsl_u_train_results\n",
    "# models_evaluation_test['knn_bsl_u']  = knn_bsl_u_test_results\n",
    "\n",
    "# models_evaluation_test['knn_bsl_u'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 KNN with Item Item similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is pretrained and saved in the directory spedified....\n",
      "Getting it from the disk...(both predicted_ratings and algorithm...)\n",
      "Done...\n",
      "loading rmse and mape values from disk...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bsl_algo': {'mape': 30.90283402482485, 'rmse': 1.0052001851833883},\n",
       " 'knn_bsl_m': {'mape': 30.14567877982246, 'rmse': 0.9984568872852136}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile('sample/large/knn_bsl_m'):\n",
    "    \n",
    "    print('This model is pretrained and saved in the directory spedified....')\n",
    "\n",
    "    ## get the predicted ratings and the algorithm itself from the disk\n",
    "    print('Getting it from the disk...(both predicted_ratings and algorithm...)')\n",
    "    knn_bsl_m_test_pred_ratings , knn_bsl_m = dump.load('sample/large/knn_bsl_m')\n",
    "    print('Done...')\n",
    "    \n",
    "    print('loading rmse and mape values from disk...')\n",
    "    with open('sample/large/metrics_knn_bsl_m','rb') as f:\n",
    "        models_evaluation_test['knn_bsl_m'] = pickle.load(f)\n",
    "\n",
    "models_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Estimating biases using sgd...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Done. time taken : 1:15:13.788940 \n",
      "\n",
      "\n",
      " we are skipping model evaluation with train data..\n",
      "\n",
      "\n",
      "Evaluating for test data...\n",
      "time taken : 1:08:19.145859\n",
      "---------------\n",
      "Test Data\n",
      "---------------\n",
      "RMSE : 0.9984568872852136\n",
      "\n",
      "MAPE : 30.14567877982246\n",
      "\n",
      "storing the test results in test dictionary...\n",
      "\n",
      "---------------------------------------------\n",
      "Total time taken to run this algorithm : 2:23:32.935913\n",
      "Storing the trained model (with predicted ratings) in the disk..\n",
      "The dump has been saved as file sample/large/knn_bsl_m\n",
      "Done..\n"
     ]
    }
   ],
   "source": [
    "# # we specify , how to compute similarities and what to consider with sim_options to our algorithm\n",
    "# sim_options = {'user_based' : False,\n",
    "#                'name': 'pearson_baseline',\n",
    "#                'shrinkage': 100,\n",
    "#                'min_support': 2\n",
    "#               } \n",
    "# # we keep other parameters like regularization parameter and learning_rate as default values.\n",
    "# bsl_options = {'method': 'sgd'} \n",
    "\n",
    "# knn_bsl_m = KNNBaseline(k=40, sim_options = sim_options, bsl_options = bsl_options)\n",
    "\n",
    "# knn_bsl_m_train_results, knn_bsl_m_test_results = run_surprise(knn_bsl_m, trainset, testset, verbose=True,\n",
    "#                                                                evaluate_train=False)\n",
    "\n",
    "# models_evaluation_train['knn_bsl_m'] = knn_bsl_m_train_results\n",
    "# models_evaluation_test['knn_bsl_m']  = knn_bsl_m_test_results\n",
    "\n",
    "# # store the trained model in disk...\n",
    "# print('Storing the trained model (with predicted ratings) in the disk..')\n",
    "# dump.dump('sample/large/knn_bsl_m', predictions=models_evaluation_test['knn_bsl_m']['predictions'],\n",
    "#           algo=knn_bsl_m, verbose=1)\n",
    "# print('Done..')\n",
    "# # saving test results in dictionary in python\n",
    "# models_evaluation_test['knn_bsl_m'] = {'rmse':knn_bsl_m_test_results['rmse'],\n",
    "#                                        'mape':knn_bsl_m_test_results['mape']}\n",
    "\n",
    "# with open('sample/large/metrics_knn_bsl_m', mode='wb') as f:\n",
    "#     pickle.dump(models_evaluation_test['knn_bsl_m'], f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Matrix Factorization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 SVD -  MF algorithm with user item interactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ Predicted Rating : __\n",
    "    - $ \\large \\hat r_{ui} = \\mu + b_u + b_i + q_i^Tp_u $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Optimization problem with user item interactions and regularization (to avoid overfitting)__\n",
    "\n",
    "\n",
    "- $\\Large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
    "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is pretrained and saved in the directory spedified....\n",
      "Getting svd from the disk...(both predicted_ratings and algorithm...)\n",
      "Done...\n",
      "\n",
      "Getting rmse and mape fom disk...\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bsl_algo': {'mape': 30.90283402482485, 'rmse': 1.0052001851833883},\n",
       " 'knn_bsl_m': {'mape': 30.14567877982246, 'rmse': 0.9984568872852136},\n",
       " 'svd': {'mape': 29.491243711503127, 'rmse': 0.9868320443529861}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import dump\n",
    "\n",
    "if os.path.isfile('sample/large/svd'):\n",
    "    \n",
    "    print('This model is pretrained and saved in the directory spedified....')\n",
    "    \n",
    "    ## get the predicted ratings and the algorithm itself from the disk\n",
    "    print('Getting svd from the disk...(both predicted_ratings and algorithm...)')\n",
    "    svd_test_pred_ratings , svd = dump.load('sample/large/svd')\n",
    "    print('Done...\\n')\n",
    "    \n",
    "    print('Getting rmse and mape fom disk...')\n",
    "    with open('sample/large/metrics_svd','rb') as f:\n",
    "        models_evaluation_test['svd'] = pickle.load(f)\n",
    "    print('Done')\n",
    "    \n",
    "models_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing the trained svd along with the predictions..\n",
      "The dump has been saved as file sample/large/svd\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "####################################################################################################\n",
    "# print('We have to train the model from test dataset. It may take a while..(90 min)')\n",
    "\n",
    "# svd = SVD(n_factors=100, n_epochs=20, random_state=15, verbose=True)\n",
    "\n",
    "\n",
    "# svd_train_results, svd_test_results = run_surprise(svd, trainset, testset,\n",
    "#                                                    evaluate_train=False, verbose=True)\n",
    "\n",
    "# # store them in models dictionary..\n",
    "# models_evaluation_train['svd'] = svd_train_results\n",
    "# models_evaluation_test['svd'] = svd_test_results\n",
    "\n",
    "# store the trained svd model in disk...(along with the predicted ratings...)\n",
    "# print('storing the trained svd along with the predictions..')\n",
    "# dump.dump('sample/large/svd', predictions=models_evaluation_test['svd']['predictions'],\n",
    "#           algo=svd, verbose=1)\n",
    "\n",
    "# # saving test results in dictionary in python\n",
    "# models_evaluation_test['svd'] = {'rmse':svd_test_results['rmse'],\n",
    "#                                  'mape':svd_test_results['mape']}\n",
    "\n",
    "# with open('sample/large/metrics_svd', mode='wb') as f:\n",
    "#     pickle.dump(models_evaluation_test['svd'], f)\n",
    "####################################################################################################\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.2 SVD -  with Implicit feedback of Items(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from surprise import SVDpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ Predicted Rating : __ \n",
    "    - $ \\large \\hat{r}_{ui} = \\mu + b_u + b_i + q_i^T\\left(p_u +\n",
    "    |I_u|^{-\\frac{1}{2}} \\sum_{j \\in I_u}y_j\\right) $ \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - $ \\pmb{I_u}$ --- the set of all items rated by user u\n",
    "\n",
    "- $\\pmb{y_j}$ --- Our new set of item factors that capture implicit ratings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Optimization problem with user item interactions and regularization (to avoid overfitting)__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "- $ \\Large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
    "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2 + y_j^2\\right) $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svdpp = SVDpp(n_factors=20, random_state=15, verbose=True)\n",
    "# svdpp_train_results, svdpp_test_results = run_surprise(svdpp, trainset, testset)\n",
    "\n",
    "# models_evaluation_train['svdpp'] = svdpp_train_results\n",
    "# models_evaluation_test['svdpp'] = svdpp_test_results\n",
    "\n",
    "# models_evaluation_test['svdpp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data metrics of models that we have trained so far "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bsl_algo</th>\n",
       "      <th>knn_bsl_m</th>\n",
       "      <th>svd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mape</th>\n",
       "      <td>30.902834</td>\n",
       "      <td>30.145679</td>\n",
       "      <td>29.491244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.005200</td>\n",
       "      <td>0.998457</td>\n",
       "      <td>0.986832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bsl_algo  knn_bsl_m        svd\n",
       "mape  30.902834  30.145679  29.491244\n",
       "rmse   1.005200   0.998457   0.986832"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(models_evaluation_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - .\n",
    " \n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - .\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final thoughts on the problem :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > We didn't do KNN with User_User similarity and SVD++, because of memory or/and time constraints.\n",
    "\n",
    "\n",
    "- > But we already proved that ( with small and medium samples ), if we blend all the models (including svd++ and Knn_UU_sim), with XGBoost, we can __furthur reduce the rmse__. \n",
    "\n",
    "- > If we do that, I am very much confident that we can get __rmse of our final model__ less than __0.9514 (rmse of Cinematch)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
