{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just to know how much time will it take to run this entire ipython notebook \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading data from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', sep=',', \n",
    "                       names=['movie', 'user','rating','date'])\n",
    "df.date = pd.to_datetime(df.date)\n",
    "\n",
    "# we are arranging the ratings according to time.\n",
    "df.sort_values(by='date', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there any entry that has Nan values...??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95880696</th>\n",
       "      <td>16992</td>\n",
       "      <td>962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95963294</th>\n",
       "      <td>17002</td>\n",
       "      <td>51082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie   user  rating date\n",
       "95880696  16992    962     NaN  NaT\n",
       "95963294  17002  51082     NaN  NaT"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it returns True for a row that has atleast one Nan value.\n",
    "null_bool = df.isnull().any(axis = 1)\n",
    "df[null_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[null_bool].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Nan values in our dataframe :  0\n"
     ]
    }
   ],
   "source": [
    "print(\"No of Nan values in our dataframe : \", sum(df.isnull().any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to find the duplicates if any..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1211 duplicate rating entries in the data..\n"
     ]
    }
   ],
   "source": [
    "dup_bool = df.duplicated(['movie','user','rating'])\n",
    "dups = sum(dup_bool) # by considering all columns..( including timestamp )\n",
    "print(\"There are {} duplicate rating entries in the data..\".format(dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16969    762\n",
       "16968    449\n",
       "Name: movie, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's find what are the movies that has duplicate entries of user ratings....\n",
    "df[dup_bool].movie.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Removing those duplicate entries__  (inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[dup_bool].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of duplicate rows (movie, user, rating) entries : 0\n"
     ]
    }
   ],
   "source": [
    "print(\"No of duplicate rows (movie, user, rating) entries :\", sum(df.duplicated()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No of ratings that we have, after removing nan rows and duplicates are : 100479045\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNo of ratings that we have, after removing nan rows and duplicates are :\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No. of Ratings, Users and Movies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data \n",
      "--------------------------------------------------\n",
      "\n",
      "Total no of ratings : 100479045\n",
      "Total No of Users   : 480189\n",
      "Total No of movies  : 17770\n"
     ]
    }
   ],
   "source": [
    "movies = df.movie.value_counts()\n",
    "users = df.user.value_counts()\n",
    "ratings = df.rating\n",
    "print(\"Total data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal no of ratings :\",df.shape[0])\n",
    "print(\"Total No of Users   :\", len(users))\n",
    "print(\"Total No of movies  :\", len(movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_train_df = df.iloc[:int(df.shape[0]*0.80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80383236, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58698779</th>\n",
       "      <td>10774</td>\n",
       "      <td>510180</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96212476</th>\n",
       "      <td>17064</td>\n",
       "      <td>510180</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1999-11-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie    user  rating       date\n",
       "58698779  10774  510180     3.0 1999-11-11\n",
       "96212476  17064  510180     2.0 1999-11-11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data \n",
      "--------------------------------------------------\n",
      "\n",
      "Total no of ratings : 80383236\n",
      "Total No of Users   : 405024\n",
      "Total No of movies  : 17423\n"
     ]
    }
   ],
   "source": [
    "movies = big_train_df.movie.value_counts()\n",
    "users = big_train_df.user.value_counts()\n",
    "print(\"Training data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal no of ratings :\",big_train_df.shape[0])\n",
    "print(\"Total No of Users   :\", len(users))\n",
    "print(\"Total No of movies  :\", len(movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating sparse matrix for Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in your pwd, getting it from disk....\n",
      "DONE..\n",
      "0:00:03.212726\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy import sparse\n",
    "start = datetime.now()\n",
    "if os.path.isfile('sample/train_sparse_matrix.npz'):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    train_sparse_matrix = sparse.load_npz('sample/train_sparse_matrix.npz')\n",
    "    print(\"DONE..\")\n",
    "else: \n",
    "    print(\"We are creating sparse_matrix from the dataframe..\")\n",
    "    # create sparse_matrix and store it for after usage.\n",
    "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
    "    # It should be in such a way that, MATRIX[row, col] = data\n",
    "    train_sparse_matrix = sparse.csr_matrix((big_train_df.rating.values, (big_train_df.user.values,\n",
    "                                               big_train_df.movie.values)))\n",
    "    \n",
    "    print('Done. It\\'s shape is : (user, movie) : ',train_sparse_matrix.shape)\n",
    "    print('Saving it into disk for furthur usage..')\n",
    "    # save it into disk\n",
    "    sparse.save_npz(\"sample/train_sparse_matrix.npz\", train_sparse_matrix)\n",
    "    print('Done..\\n')\n",
    " \n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,m = train_sparse_matrix.shape\n",
    "elem = train_sparse_matrix.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Of matrix : 99.82927583214679 % \n"
     ]
    }
   ],
   "source": [
    "print(\"Sparsity Of matrix : {} % \".format(  (1-(elem/(u*m))) * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_test_df = df.iloc[int(df.shape[0]*0.80) : ]\n",
    "big_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52863848</th>\n",
       "      <td>9617</td>\n",
       "      <td>316390</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2005-08-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12989568</th>\n",
       "      <td>2462</td>\n",
       "      <td>605375</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-08-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie    user  rating       date\n",
       "52863848   9617  316390     2.0 2005-08-08\n",
       "12989568   2462  605375     4.0 2005-08-08"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data \n",
      "--------------------------------------------------\n",
      "\n",
      "Total no of ratings : 20095809\n",
      "Total No of Users   : 349327\n",
      "Total No of movies  : 17757\n"
     ]
    }
   ],
   "source": [
    "movies = big_test_df.movie.value_counts()\n",
    "users = big_test_df.user.value_counts()\n",
    "\n",
    "print(\"Test data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal no of ratings :\",big_test_df.shape[0])\n",
    "print(\"Total No of Users   :\", len(users))\n",
    "print(\"Total No of movies  :\", len(movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating sparse matrix for Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in your pwd, getting it from disk....\n",
      "DONE..\n",
      "0:00:00.855393\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy import sparse\n",
    "start = datetime.now()\n",
    "if os.path.isfile('sample/test_sparse_matrix.npz'):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    test_sparse_matrix = sparse.load_npz('sample/test_sparse_matrix.npz')\n",
    "    print(\"DONE..\")\n",
    "else: \n",
    "    print(\"We are creating sparse_matrix from the dataframe..\")\n",
    "    # create sparse_matrix and store it for after usage.\n",
    "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
    "    # It should be in such a way that, MATRIX[row, col] = data\n",
    "    test_sparse_matrix = sparse.csr_matrix((big_test_df.rating.values, (big_test_df.user.values,\n",
    "                                               big_test_df.movie.values)))\n",
    "    \n",
    "    print('Done. It\\'s shape is : (user, movie) : ',test_sparse_matrix.shape)\n",
    "    print('Saving it into disk for furthur usage..')\n",
    "    # save it into disk\n",
    "    sparse.save_npz(\"sample/test_sparse_matrix.npz\", test_sparse_matrix)\n",
    "    print('Done..\\n')\n",
    " \n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,m = test_sparse_matrix.shape\n",
    "elem = test_sparse_matrix.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20095713"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Of matrix : 99.95731855608713 % \n"
     ]
    }
   ],
   "source": [
    "print(\"Sparsity Of matrix : {} % \".format(  (1-(elem/(u*m))) * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Average Ratings (from Train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean matrix of ratings ( whether a user rated that movie or not)\n",
    "is_rated = train_sparse_matrix!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': 3.5829128738184792}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the global average of ratings in our train set.\n",
    "global_average = train_sparse_matrix.sum()/train_sparse_matrix.count_nonzero()\n",
    "averages['global'] = global_average\n",
    "averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.182377049180328"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the user averages in dictionary (key: userid, value: avg rating)\n",
    "#_____________________________________________________________________#\n",
    "\n",
    "# \".A1\" is for converting Column_Matrix to 1-D numpy array \n",
    "sum_of_ratings_per_user = train_sparse_matrix.sum(axis=1).A1\n",
    "# no of ratings that each user has given.\n",
    "no_of_ratings_per_user = is_rated.sum(axis=1).A1\n",
    "\n",
    "# creae a dictonary of users and their average ratigns..\n",
    "average_user_ratings = { i : sum_of_ratings_per_user[i]/no_of_ratings_per_user[i]  \n",
    "                                 for i in range(train_sparse_matrix.shape[0]) \n",
    "                                    if no_of_ratings_per_user[i] !=0}\n",
    "\n",
    "# add user averages to th eaverages dictionary\n",
    "averages['user'] = average_user_ratings\n",
    "\n",
    "# test it..\n",
    "averages['user'][97]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7808264297834113"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the Movie Average ratings in dictionary (key: movieId, value: avg_rating)\n",
    "#_____________________________________________________________________#\n",
    "\n",
    "# sum of the ratings that a movie got by any user(who rated that movie..)\n",
    "sum_of_ratings_per_movie = train_sparse_matrix.sum(axis=0).A1\n",
    "# no of ratings that a movie got.\n",
    "no_of_ratings_per_movie = is_rated.sum(axis=0).A1\n",
    "\n",
    "average_movie_ratings = {i : sum_of_ratings_per_movie[i]/ no_of_ratings_per_movie[i] \n",
    "                                for i in range(train_sparse_matrix.shape[1])\n",
    "                                    if no_of_ratings_per_movie[i]!=0 }\n",
    "\n",
    "# add thie'per_movie' avg ratings to averages dictionary\n",
    "averages['movie'] = average_movie_ratings\n",
    "\n",
    "# test this dictionary\n",
    "averages['movie'][30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Trainset and testset for Surprise based alorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading it from the disk\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "if os.path.isfile('sample/large/trainset.pickle'):\n",
    "    print('loading it from the disk')\n",
    "    trainset = pickle.load(open('sample/large/trainset.pickle', 'rb'))\n",
    "    print('done')\n",
    "else:\n",
    "    print('creating it from sparse_matrix ( if it is loaded)')\n",
    "    \n",
    "    train_users, train_movies, train_ratings = sparse.find(train_sparse_matrix)\n",
    "    \n",
    "   \n",
    "\n",
    "    print('preparing train dataframe with users, movies and ratings of the trainset..')\n",
    "    surp_train = pd.DataFrame({'user': train_users,\n",
    "                           'movie': train_movies,\n",
    "                           'rating': train_ratings}, )\n",
    "    surp_train = surp_train[['user','movie','rating']]\n",
    "    print(surp_train.head(2))\n",
    "    \n",
    "    print('Creating trainset from the dataframe...')\n",
    "    trainset = Dataset.load_from_df(surp_train,Reader(rating_scale=(1,5))).build_full_trainset()\n",
    "    \n",
    "    print('No of unique users, unique movies and ratings in train data', end=' : ')\n",
    "    print('(users, movies, ratings) : ({}, {}, {})'.format(trainset.n_users, \n",
    "                                                           trainset.n_items,\n",
    "                                                           trainset.n_ratings))\n",
    "    \n",
    "    # saving testset to dsik\n",
    "    print('saving it to disk..')\n",
    "    start = datetime.now()\n",
    "    with open('sample/large/trainset.pickle', 'wb') as f:\n",
    "        pickle.dump(trainset, f)\n",
    "    print(datetime.now() - start)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405024, 17423, 80382095)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.n_users, trainset.n_items, trainset.n_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testset from the disk..\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "if os.path.isfile('sample/large/testset.pickle'):\n",
    "    print('loading testset from the disk..')\n",
    "    testset = pickle.load(open('sample/large/testset.pickle', 'rb'))\n",
    "    print('Done')\n",
    "else:\n",
    "    print('creating tesetset from sparse matrix.( if it is loaded )')\n",
    "    \n",
    "    test_users, test_movies, test_ratings = sparse.find(test_sparse_matrix)\n",
    "    \n",
    "    print('No of unique users, unique movies..', end=' : ')\n",
    "    print(len(np.unique(test_users)),  len(np.unique(test_movies)))\n",
    "\n",
    "    print(\"No of ratings in test set :\",len(test_ratings))\n",
    "\n",
    "    testset = list(zip(test_users, test_movies, test_ratings))\n",
    "\n",
    "    # saving testset to dsik\n",
    "    print('saving it to disk..')\n",
    "    start = datetime.now()\n",
    "    with open('sample/large/testset.pickle', 'wb') as f:\n",
    "        pickle.dump(testset, f)\n",
    "    print(datetime.now() - start)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic function to run any surprise based algorithm \n",
    "\n",
    "    - given prefectly initialized ALGO, TRAINSET and TESTSET\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "##########################################################\n",
    "# get  (actual_list , predicted_list) ratings given list \n",
    "# of predictions (prediction is a class in Surprise).    \n",
    "##########################################################\n",
    "def get_ratings(predictions):\n",
    "    actual = np.array([pred.r_ui for pred in predictions])\n",
    "    pred = np.array([pred.est for pred in predictions])\n",
    "    \n",
    "    return actual, pred\n",
    "\n",
    "################################################################\n",
    "# get ''rmse'' and ''mape'' , given list of prediction classes \n",
    "################################################################\n",
    "def get_errors(predictions, print_them=False):\n",
    "\n",
    "    actual, pred = get_ratings(predictions)\n",
    "    rmse = np.sqrt(np.mean(pred - actual)**2)\n",
    "    mape = np.mean(np.abs(pred - actual)/actual)\n",
    "\n",
    "    return rmse, mape*100\n",
    "\n",
    "##################################################################################\n",
    "# It will return predicted ratings, rmse and mape of both train and test data   #\n",
    "##################################################################################\n",
    "def run_surprise(algo, trainset, testset,evaluate_train=False, verbose=True): \n",
    "    '''\n",
    "        return train_dict, test_dict\n",
    "    \n",
    "        It returns two dictionaries, one for train and the other is for test\n",
    "        Each of them have 3 key-value pairs, which specify ''rmse'', ''mape'', and ''predicted ratings''.\n",
    "    '''\n",
    "    start = datetime.now()\n",
    "    # dictionaries that stores metrics for train and test..\n",
    "    train = dict()\n",
    "    test = dict()\n",
    "    \n",
    "    # train the algorithm with the trainset\n",
    "    st = datetime.now()\n",
    "    print('Training the model...')\n",
    "    algo.fit(trainset)\n",
    "    print('Done. time taken : {} \\n'.format(datetime.now()-st))\n",
    "    \n",
    "    # ---------------- Evaluating train data--------------------#\n",
    "    if evaluate_train:\n",
    "        st = datetime.now()\n",
    "        print('Evaluating the model with train data..')\n",
    "        # get the train predictions (list of prediction class inside Surprise)\n",
    "        train_preds = algo.test(trainset.build_testset())\n",
    "        # get predicted ratings from the train predictions..\n",
    "        train_actual_ratings, train_pred_ratings = get_ratings(train_preds)\n",
    "        # get ''rmse'' and ''mape'' from the train predictions.\n",
    "        train_rmse, train_mape = get_errors(train_preds)\n",
    "        print('time taken : {}'.format(datetime.now()-st))\n",
    "        if verbose:\n",
    "            print('-'*15)\n",
    "            print('Train Data')\n",
    "            print('-'*15)\n",
    "            print(\"RMSE : {}\\n\\nMAPE : {}\\n\".format(train_rmse, train_mape))\n",
    "    \n",
    "        #store them in the train dictionary\n",
    "        if verbose:\n",
    "            print('adding train results in the dictionary..')\n",
    "        train['rmse'] = train_rmse\n",
    "        train['mape'] = train_mape\n",
    "        train['predictions'] = train_pred_ratings\n",
    "    else:\n",
    "        print('\\n we are skipping model evaluation with train data..\\n')\n",
    "        train = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    #------------ Evaluating Test data---------------#\n",
    "    st = datetime.now()\n",
    "    print('\\nEvaluating for test data...')\n",
    "    # get the predictions( list of prediction classes) of test data\n",
    "    test_preds = algo.test(testset)\n",
    "    # get the predicted ratings from the list of predictions\n",
    "    test_actual_ratings, test_pred_ratings = get_ratings(test_preds)\n",
    "    # get error metrics from the predicted and actual ratings\n",
    "    test_rmse, test_mape = get_errors(test_preds)\n",
    "    print('time taken : {}'.format(datetime.now()-st))\n",
    "    \n",
    "    if verbose:\n",
    "        print('-'*15)\n",
    "        print('Test Data')\n",
    "        print('-'*15)\n",
    "        print(\"RMSE : {}\\n\\nMAPE : {}\\n\".format(test_rmse, test_mape))\n",
    "    # store them in test dictionary\n",
    "    if verbose:\n",
    "        print('storing the test results in test dictionary...')\n",
    "    test['rmse'] = test_rmse\n",
    "    test['mape'] = test_mape\n",
    "    test['predictions'] = test_pred_ratings\n",
    "    \n",
    "    print('\\n'+'-'*45)\n",
    "    print('Total time taken to run this algorithm :', datetime.now() - start)\n",
    "    \n",
    "    # return two dictionaries train and test\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Global dictionary that stores rmse and mape for all the models....\n",
    "\n",
    "- It stores the metrics in a dictionary of dictionaries\n",
    "\n",
    "> __keys__ : model names(string)\n",
    "\n",
    "> __value__: dict(__key__ : metric, __value__ : value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_evaluation_train = dict()\n",
    "models_evaluation_test = dict()\n",
    "\n",
    "models_evaluation_train, models_evaluation_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline Model  ( with User and Item biases )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import BaselineOnly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Predicted_rating : ( baseline prediction )\n",
    "\n",
    ">$   \\large {\\hat{r}_{ui} = b_{ui} =\\mu + b_u + b_i} $\n",
    "\n",
    "- ####  Optimization function ( Least Squares Problem )\n",
    "\n",
    "> $ \\large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - (\\mu + b_u + b_i)\\right)^2 +\n",
    "\\lambda \\left(b_u^2 + b_i^2 \\right).\\text {        [mimimize } {b_u, b_i]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'sgd',\n",
    "               'learning_rate': .001\n",
    "               }\n",
    "bsl = BaselineOnly(bsl_options=bsl_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Estimating biases using sgd...\n",
      "Done. time taken : 0:02:36.793990 \n",
      "\n",
      "\n",
      " we are skipping model evaluation with train data..\n",
      "\n",
      "\n",
      "Evaluating for test data...\n",
      "time taken : 0:03:07.972872\n",
      "---------------\n",
      "Test Data\n",
      "---------------\n",
      "RMSE : 0.02624592932029108\n",
      "\n",
      "MAPE : 30.90283402482485\n",
      "\n",
      "storing the test results in test dictionary...\n",
      "\n",
      "---------------------------------------------\n",
      "Total time taken to run this algorithm : 0:05:44.767861\n"
     ]
    }
   ],
   "source": [
    "bsl_train_results, bsl_test_results = run_surprise(bsl, trainset, testset,\n",
    "                                                   evaluate_train=False ,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mape': 30.90283402482485,\n",
       " 'predictions': array([4.04593075, 3.06951653, 3.9550621 , ..., 2.4852953 , 2.89735459,\n",
       "        2.89735459]),\n",
       " 'rmse': 0.02624592932029108}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store them in models dictionary..\n",
    "models_evaluation_train['bsl'] = bsl_train_results\n",
    "models_evaluation_test['bsl'] = bsl_test_results\n",
    "\n",
    "models_evaluation_test['bsl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dump has been saved as file bsl_algo\n"
     ]
    }
   ],
   "source": [
    "# store the trained model in disk...\n",
    "from surprise import dump\n",
    "\n",
    "dump.dump('sample/large/bsl_algo', predictions=models_evaluation_test['bsl']['predictions'],\n",
    "          algo=bsl, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. KNN with Baseline_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBaseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __predicted Rating__ : ( ___ based on User-User similarity ___ )\n",
    "\n",
    "\\begin{align} \\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
    "\\text{sim}(u, v) \\cdot (r_{vi} - b_{vi})} {\\sum\\limits_{v \\in\n",
    "N^k_i(u)} \\text{sim}(u, v)} \\end{align}\n",
    "\n",
    "- $\\pmb{b_{ui}}$ -  _Baseline prediction_ of (user,movie) rating\n",
    "\n",
    "\n",
    "- $ \\pmb {N_i^k (u)}$ - Set of __K similar__ users (neighbours) of __user (u)__ who rated __movie(i)__  \n",
    "\n",
    "\n",
    "- _sim (u, v)_ - __Similarity__ between users __u and v__  \n",
    "    - Generally, it will be cosine similarity or Pearson correlation coefficient. \n",
    "    - But we use __shrunk Pearson-baseline correlation coefficient__, which is based on the pearsonBaseline similarity ( we take base line predictions instead of mean rating of user/item)\n",
    "        - Computation of the correlation coefficient is based only on the common user support.\n",
    "        - similarities based on a greater user support are more reliable ie., Users who has more number of common movie ratings are considered as more similar than users who has few no of movies  in common which are rated.\n",
    "        - \\begin{align}\\begin{aligned}\\text{pearson_baseline_shrunk_sim}(u, v) &= \\frac{|I_{uv}| - 1}{|I_{uv}| - 1 + \\text{shrinkage}} \\cdot \\hat{\\rho}_{uv}\\end{aligned}\\end{align}\n",
    "            - $\\pmb{|I_{uv}|}$ - No of common movies between users(u and v)\n",
    "            - __shrinkage__ - kind of hyperparameter. The defalut value suggested is ___100___\n",
    "                - __0__ : There is no shrinkage at all ( It is normal pearson correlation coefficient ) \n",
    "            - $ \\pmb {\\hat \\rho_uv}$ - Pearson Correlation Coefficient ( between users )\n",
    "                - \\begin{align} \\text{pearson_baseline_sim}(u, v) = \\hat{\\rho}_{uv} = \\frac{\n",
    "    \\sum\\limits_{i \\in I_{uv}} (r_{ui} -  b_{ui}) \\cdot (r_{vi} -\n",
    "    b_{vi})} {\\sqrt{\\sum\\limits_{i \\in I_{uv}} (r_{ui} -  b_{ui})^2}\n",
    "    \\cdot \\sqrt{\\sum\\limits_{i \\in I_{uv}} (r_{vi} -  b_{vi})^2}} \\end{align}\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ------------ or \n",
    " --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ Predicted rating __ ( based on Item Item similarity ):\n",
    " \\begin{align} \\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{j \\in N^k_u(i)}\\text{sim}(i, j) \\cdot (r_{uj} - b_{uj})} {\\sum\\limits_{j \\in N^k_u(j)} \\text{sim}(i, j)} \\end{align}\n",
    "\n",
    "    -  ___Notations follows same as above (user user based predicted rating ) ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can do both and blend them ( see if we can better results when combined ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 KNN with User User similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we specify , how to compute similarities and what to consider with sim_options to our algorithm\n",
    "sim_options = {'user_based' : True,\n",
    "               'name': 'pearson_baseline',\n",
    "               'shrinkage': 100,\n",
    "               'min_support': 2\n",
    "              } \n",
    "\n",
    "bsl_options = {'method': 'sgd'} # we keep other parameters like regularization parameter and learning_rate as default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_bsl_u = KNNBaseline(k=40, sim_options = sim_options, bsl_options = bsl_options)\n",
    "\n",
    "knn_bsl_u_train_results, knn_bsl_u_test_results = run_surprise(knn_bsl_u, trainset, testset, \n",
    "                                                               evaluate_train=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_evaluation_train['knn_bsl_u'] = knn_bsl_u_train_results\n",
    "models_evaluation_test['knn_bsl_u']  = knn_bsl_u_test_results\n",
    "\n",
    "models_evaluation_test['knn_bsl_u'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 KNN with Item Item similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we specify , how to compute similarities and what to consider with sim_options to our algorithm\n",
    "sim_options = {'user_based' : False,\n",
    "               'name': 'pearson_baseline',\n",
    "               'shrinkage': 100,\n",
    "               'min_support': 2\n",
    "              } \n",
    "\n",
    "bsl_options = {'method': 'sgd'} # we keep other parameters like regularization parameter and learning_rate as default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_bsl_m = KNNBaseline(k=40, sim_options = sim_options, bsl_options = bsl_options)\n",
    "\n",
    "knn_bsl_m_train_results, knn_bsl_m_test_results = run_surprise(knn_bsl_m, trainset, testset, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_evaluation_train['knn_bsl_m'] = knn_bsl_m_train_results\n",
    "models_evaluation_test['knn_bsl_m']  = knn_bsl_m_test_results\n",
    "\n",
    "models_evaluation_test['knn_bsl_m'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Matrix Factorization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 SVD -  MF algorithm with user item interactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ Predicted Rating : __\n",
    "    - $ \\large \\hat r_{ui} = \\mu + b_u + b_i + q_i^Tp_u $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Optimization problem with user item interactions and regularization (to avoid overfitting)__\n",
    "\n",
    "\n",
    "- $\\Large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
    "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Done. time taken : 1:15:35.637917 \n",
      "\n",
      "\n",
      " we are skipping model evaluation with train data..\n",
      "\n",
      "\n",
      "Evaluating for test data...\n",
      "time taken : 0:03:43.571852\n",
      "---------------\n",
      "Test Data\n",
      "---------------\n",
      "RMSE : 0.009300209925620563\n",
      "\n",
      "MAPE : 29.491243711503127\n",
      "\n",
      "storing the test results in test dictionary...\n",
      "\n",
      "---------------------------------------------\n",
      "Total time taken to run this algorithm : 1:19:19.209769\n"
     ]
    }
   ],
   "source": [
    "svd = SVD(n_factors=100, n_epochs=20, random_state=15, verbose=True)\n",
    "svd_train_results, svd_test_results = run_surprise(svd, trainset, testset,\n",
    "                                                   evaluate_train=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mape': 29.491243711503127,\n",
       " 'predictions': array([3.97526121, 2.55296855, 3.63684438, ..., 2.58194006, 2.80058386,\n",
       "        2.80058386]),\n",
       " 'rmse': 0.009300209925620563}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_evaluation_train['svd'] = svd_train_results\n",
    "models_evaluation_test['svd'] = svd_test_results\n",
    "\n",
    "models_evaluation_test['svd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dump has been saved as file sample/large/svd\n"
     ]
    }
   ],
   "source": [
    "# store the trained svd model in disk...\n",
    "from surprise import dump\n",
    "\n",
    "dump.dump('sample/large/svd', predictions=models_evaluation_test['bsl']['predictions'],\n",
    "          algo=bsl, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.2 SVD -  with Implicit feedback of Items(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVDpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ Predicted Rating : __ \n",
    "    - $ \\large \\hat{r}_{ui} = \\mu + b_u + b_i + q_i^T\\left(p_u +\n",
    "    |I_u|^{-\\frac{1}{2}} \\sum_{j \\in I_u}y_j\\right) $ \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - $ \\pmb{I_u}$ --- the set of all items rated by user u\n",
    "\n",
    "- $\\pmb{y_j}$ --- Our new set of item factors that capture implicit ratings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Optimization problem with user item interactions and regularization (to avoid overfitting)__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "- $ \\Large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
    "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2 + y_j^2\\right) $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      " processing epoch 0\n",
      " processing epoch 1\n",
      " processing epoch 2\n"
     ]
    }
   ],
   "source": [
    "svdpp = SVDpp(n_factors=20, random_state=15, verbose=True)\n",
    "svdpp_train_results, svdpp_test_results = run_surprise(svdpp, trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mape': 35.31675253673884,\n",
       " 'predictions': array([3.91771285, 3.7912363 , 3.43484015, ..., 3.60432926, 3.51151561,\n",
       "        3.4300295 ]),\n",
       " 'rmse': 0.2496661363739624}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_evaluation_train['svdpp'] = svdpp_train_results\n",
    "models_evaluation_test['svdpp'] = svdpp_test_results\n",
    "\n",
    "models_evaluation_test['svdpp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bsl</th>\n",
       "      <th>knn_bsl_m</th>\n",
       "      <th>knn_bsl_u</th>\n",
       "      <th>svd</th>\n",
       "      <th>svdpp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mape</th>\n",
       "      <td>34.6112</td>\n",
       "      <td>36.3129</td>\n",
       "      <td>36.3129</td>\n",
       "      <td>35.3592</td>\n",
       "      <td>35.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictions</th>\n",
       "      <td>[4.036206096101787, 3.6381296161606014, 3.5320...</td>\n",
       "      <td>[4.460806391864557, 3.4690056553542377, 3.4666...</td>\n",
       "      <td>[4.460806391864557, 3.4690056553542377, 3.4666...</td>\n",
       "      <td>[4.12340143043477, 3.719840558294905, 3.438442...</td>\n",
       "      <td>[3.9177128477107477, 3.7912363045212905, 3.434...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.177168</td>\n",
       "      <td>0.259568</td>\n",
       "      <td>0.259568</td>\n",
       "      <td>0.229293</td>\n",
       "      <td>0.249666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           bsl  \\\n",
       "mape                                                   34.6112   \n",
       "predictions  [4.036206096101787, 3.6381296161606014, 3.5320...   \n",
       "rmse                                                  0.177168   \n",
       "\n",
       "                                                     knn_bsl_m  \\\n",
       "mape                                                   36.3129   \n",
       "predictions  [4.460806391864557, 3.4690056553542377, 3.4666...   \n",
       "rmse                                                  0.259568   \n",
       "\n",
       "                                                     knn_bsl_u  \\\n",
       "mape                                                   36.3129   \n",
       "predictions  [4.460806391864557, 3.4690056553542377, 3.4666...   \n",
       "rmse                                                  0.259568   \n",
       "\n",
       "                                                           svd  \\\n",
       "mape                                                   35.3592   \n",
       "predictions  [4.12340143043477, 3.719840558294905, 3.438442...   \n",
       "rmse                                                  0.229293   \n",
       "\n",
       "                                                         svdpp  \n",
       "mape                                                   35.3168  \n",
       "predictions  [3.9177128477107477, 3.7912363045212905, 3.434...  \n",
       "rmse                                                  0.249666  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(models_evaluation_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
