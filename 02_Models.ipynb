{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Building some simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will build a simple model whuch uses the similarity measures.\n",
    "\n",
    "\n",
    "- It is ___ might be worse than___ any of the basic prediction algorithms. (__like, THE BASELINE MODEL__, with biases)\n",
    "\n",
    "\n",
    "- It is just to show that, How we can use __similarity matrices__ while recommending OR predicting the user's rating/interest in some perticular movie/item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 1.  Get the training_sparse matrix from the disk__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading it from disk..\n",
      "Our similarity matrix is of (2649430, 17771)  size\n"
     ]
    }
   ],
   "source": [
    "# let's read movie_movie similarity matrix from the disk if if it is present\n",
    "import os\n",
    "from scipy import sparse\n",
    "\n",
    "if os.path.isfile('train_sparse_matrix.npz'):\n",
    "    # just read from the disk \n",
    "    print('reading it from disk..')\n",
    "    train_sparse_matrix = sparse.load_npz('train_sparse_matrix.npz')\n",
    "    print('Our similarity matrix is of',train_sparse_matrix.shape,' size')\n",
    "else:\n",
    "    # We dont't want to create one. So,\n",
    "    print(\"Oops..!, you don't have the m_m_similarity file in your directory. Get it first..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 2.  Get the SImilar_movies dict  from the disk__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading it from disk..\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# let's read movie_movie similarity matrix from the disk if if it is present\n",
    "import os\n",
    "from scipy import sparse\n",
    "\n",
    "if os.path.isfile('similar_movies.pickle'):\n",
    "    # just read from the disk \n",
    "    print('reading it from disk..')\n",
    "    with open('similar_movies.pickle', 'rb') as sm:\n",
    "        similar_movies = pickle.load(sm)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    # We dont't want to create one. So,\n",
    "    print(\"Oops..!, you don't have the file in your directory. Get it first..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  867, 11602, 13112,  9894, 13224,  7530, 12893,  2664, 14693,\n",
       "       14049,  4957, 14581,  2877,  9342,  7604,  5756,  1745,  2031,\n",
       "       12006,  1981,  8188, 12445, 16520, 12120,  5095,  5722, 10565,\n",
       "        6811,  7575,   966,  8749,  4703,  4193, 10194,  8479, 10525,\n",
       "       17392, 12575,  5866,  4760, 13942,  2025,  3774,  3038, 16326,\n",
       "        2258,  8332, 10207,  7345,   142,  3209,   141,  8319, 11729,\n",
       "         383,  8909,   744,  2084,  8362,  1538,   685, 11018,  3899,\n",
       "        5804,  2744,   979,  5491,  2501, 16595,  4322,  6039, 13026,\n",
       "       13172,  4430,  9255, 14535, 13432,  5483,  8927, 13844, 10518,\n",
       "       17237, 13913, 11950, 11807,  4937, 13308,  7484,  6694,  9444,\n",
       "       14416,  2374,   970,   126, 13338,   602,  3618,  1680, 10705,\n",
       "       16268], dtype=int64)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_movies[59]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 3. Get the averages file from the disk__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('averages.pickle'):\n",
    "    # open -------- read ------- close\n",
    "    avgs_file = open('averages.pickle', 'rb')\n",
    "    averages = pickle.load(avgs_file)\n",
    "    avgs_file.close()\n",
    "else:\n",
    "    print(\"Averages.pickle file is not presnt. Get it...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['global', 'user', 'movie'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averages.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5516949697678895"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing averages file\n",
    "averages['global']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Get the CrossValidtion dataset from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from the disk..\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('cv.csv'):\n",
    "    # read from the disk\n",
    "    print('reading from the disk..')\n",
    "    cv_df = pd.read_csv('cv.csv')\n",
    "    print('Done')\n",
    "else:\n",
    "    print(\" No cross validation file exists. Please get it..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>user</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4149</td>\n",
       "      <td>1706831</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2005-03-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6166</td>\n",
       "      <td>1821144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2005-03-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3917</td>\n",
       "      <td>1928592</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2005-03-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1798</td>\n",
       "      <td>2256821</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2005-03-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12911</td>\n",
       "      <td>94782</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2005-03-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie     user  rating        date\n",
       "0   4149  1706831     3.0  2005-03-29\n",
       "1   6166  1821144     3.0  2005-03-29\n",
       "2   3917  1928592     3.0  2005-03-29\n",
       "3   1798  2256821     3.0  2005-03-29\n",
       "4  12911    94782     3.0  2005-03-29"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cv_df.duplicated(subset=['movie','user','rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  GARSI Algorithm ---- Using MOVIE_MOVIE similarity\n",
    "\n",
    "( __G__lobal __A__verage __R__ating with __S__imilar __I__tems )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Algorithm__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\Large \\hat r_{ui} = \\frac{\\text { Global Average Rating } + \\frac {\\sum (\\text{User Ratings of similar movies})} {\\text {No of ratings }}} {2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will take __top 10__ ratings of similar movies by this user, and take average of them.\n",
    "\n",
    "\n",
    "- We will take this ( avg rating of similar movies by this user) and Global average of ratings, We again average them.\n",
    "\n",
    "\n",
    "- It is not the best model, but, it is not that bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___why top 10..? why not top 20..??___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is just for this model. Later we will build __a Regression out of this top_10 or top_20 features__ and let the model figure it out which ratings to consider.\n",
    "\n",
    "\n",
    "- We will also add some other features to this regression problem. like _user average rating_, _movie average rating_, _global average rating_, ...., ..etc.,. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is the algorithm........\n",
    "\n",
    "def average_of_top_similar_movies(similarities, ratings, top=100, verbose=False):\n",
    "    # get all similarities into an numpy array.\n",
    "    sims = similarities.toarray().ravel() if sparse.issparse(similarities) else similarities\n",
    "    # get the top similar users or items, we don't care about those values...\n",
    "    top_movies = np.argsort(sims)[::-1][1:top+1] # we are ignoring the movie/user itself..\n",
    "    \n",
    "    top_ratings = ratings.toarray().ravel() if sparse.issparse(ratings) else ratings\n",
    "    if verbose:\n",
    "        print(sum(top_ratings[top_users_or_movies] != 0))\n",
    "        print(sum(top_ratings[top_users_or_movies]))\n",
    "        print(len(top_users_or_movies))\n",
    "    # return the average rating of top ''top'' users or movies\n",
    "    return sum(top_ratings[top_users_or_movies]) / (sum(top_ratings[top_users_or_movies]!=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data from the cv_df file and predict the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = cv_df.user.values\n",
    "movies = cv_df.movie.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u,m in zip(users, movies):\n",
    "    print(u,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20095809"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 4., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0., 0., 0., 2., 0.,\n",
       "        0., 2., 3., 0., 0., 0., 0., 4., 0., 0., 0., 0., 0., 0., 0., 4.,\n",
       "        0., 0., 0., 0., 0., 4., 0., 1., 0., 0., 0., 0., 0., 5., 4., 5.,\n",
       "        0., 4., 0., 5., 1., 0., 2., 0., 0., 0., 0., 0., 0., 2., 0., 2.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4.,\n",
       "        5., 0., 0., 0., 4., 0., 0., 5., 0., 0., 1., 0., 0., 0., 3., 2.,\n",
       "        0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sparse_matrix[477189, similar_movies[886]].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16464"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similar_movies.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['global', 'user', 'movie'])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averages.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting ratings for 20095809 user_movie pairs...\n",
      "predicted for 200000 users. Time taken : 0:01:17.023165\n",
      "predicted for 400000 users. Time taken : 0:02:34.083629\n",
      "predicted for 600000 users. Time taken : 0:03:50.582331\n",
      "predicted for 800000 users. Time taken : 0:05:06.772229\n",
      "predicted for 1000000 users. Time taken : 0:06:23.748211\n",
      "predicted for 1200000 users. Time taken : 0:07:41.256943\n",
      "predicted for 1400000 users. Time taken : 0:08:56.326979\n",
      "predicted for 1600000 users. Time taken : 0:10:13.000987\n",
      "predicted for 1800000 users. Time taken : 0:11:29.491564\n",
      "predicted for 2000000 users. Time taken : 0:12:43.608944\n",
      "predicted for 2200000 users. Time taken : 0:13:58.797734\n",
      "predicted for 2400000 users. Time taken : 0:15:14.931452\n",
      "predicted for 2600000 users. Time taken : 0:16:31.591888\n",
      "predicted for 2800000 users. Time taken : 0:17:47.832156\n",
      "predicted for 3000000 users. Time taken : 0:19:04.315638\n",
      "predicted for 3200000 users. Time taken : 0:20:20.368578\n",
      "predicted for 3400000 users. Time taken : 0:21:34.052686\n",
      "predicted for 3600000 users. Time taken : 0:22:48.382803\n",
      "predicted for 3800000 users. Time taken : 0:24:04.338646\n",
      "predicted for 4000000 users. Time taken : 0:25:19.030800\n",
      "predicted for 4200000 users. Time taken : 0:26:34.792612\n",
      "predicted for 4400000 users. Time taken : 0:27:50.331560\n",
      "predicted for 4600000 users. Time taken : 0:29:06.332184\n",
      "predicted for 4800000 users. Time taken : 0:30:22.121482\n",
      "predicted for 5000000 users. Time taken : 0:31:36.861909\n",
      "predicted for 5200000 users. Time taken : 0:32:52.303960\n",
      "predicted for 5400000 users. Time taken : 0:34:06.892131\n",
      "predicted for 5600000 users. Time taken : 0:35:22.020342\n",
      "predicted for 5800000 users. Time taken : 0:36:36.289437\n",
      "predicted for 6000000 users. Time taken : 0:37:50.620165\n",
      "predicted for 6200000 users. Time taken : 0:39:05.823517\n",
      "predicted for 6400000 users. Time taken : 0:40:20.100501\n",
      "predicted for 6600000 users. Time taken : 0:41:35.986082\n",
      "predicted for 6800000 users. Time taken : 0:42:50.099774\n",
      "predicted for 7000000 users. Time taken : 0:44:05.495452\n",
      "predicted for 7200000 users. Time taken : 0:45:20.910409\n",
      "predicted for 7400000 users. Time taken : 0:46:36.341842\n",
      "predicted for 7600000 users. Time taken : 0:47:49.492888\n",
      "predicted for 7800000 users. Time taken : 0:49:03.092591\n",
      "predicted for 8000000 users. Time taken : 0:50:16.565501\n",
      "predicted for 8200000 users. Time taken : 0:51:31.736973\n",
      "predicted for 8400000 users. Time taken : 0:52:44.168844\n",
      "predicted for 8600000 users. Time taken : 0:53:59.161213\n",
      "predicted for 8800000 users. Time taken : 0:55:13.577956\n",
      "predicted for 9000000 users. Time taken : 0:56:27.328574\n",
      "predicted for 9200000 users. Time taken : 0:57:41.538925\n",
      "predicted for 9400000 users. Time taken : 0:58:56.059482\n",
      "predicted for 9600000 users. Time taken : 1:00:10.408737\n",
      "predicted for 9800000 users. Time taken : 1:01:24.049004\n",
      "predicted for 10000000 users. Time taken : 1:02:38.296626\n",
      "predicted for 10200000 users. Time taken : 1:03:52.368153\n",
      "predicted for 10400000 users. Time taken : 1:05:05.307659\n",
      "predicted for 10600000 users. Time taken : 1:06:19.286842\n",
      "predicted for 10800000 users. Time taken : 1:07:33.254078\n",
      "predicted for 11000000 users. Time taken : 1:08:47.615388\n",
      "predicted for 11200000 users. Time taken : 1:10:01.970031\n",
      "predicted for 11400000 users. Time taken : 1:11:14.994067\n",
      "predicted for 11600000 users. Time taken : 1:12:26.765839\n",
      "predicted for 11800000 users. Time taken : 1:13:40.752021\n",
      "predicted for 12000000 users. Time taken : 1:14:54.881774\n",
      "predicted for 12200000 users. Time taken : 1:16:09.299480\n",
      "predicted for 12400000 users. Time taken : 1:17:21.585227\n",
      "predicted for 12600000 users. Time taken : 1:18:35.520268\n",
      "predicted for 12800000 users. Time taken : 1:19:50.305874\n",
      "predicted for 13000000 users. Time taken : 1:21:04.425951\n",
      "predicted for 13200000 users. Time taken : 1:22:18.457397\n",
      "predicted for 13400000 users. Time taken : 1:23:32.887983\n",
      "predicted for 13600000 users. Time taken : 1:24:46.561845\n",
      "predicted for 13800000 users. Time taken : 1:25:59.557096\n",
      "predicted for 14000000 users. Time taken : 1:27:12.930453\n",
      "predicted for 14200000 users. Time taken : 1:28:24.522222\n",
      "predicted for 14400000 users. Time taken : 1:29:38.816820\n",
      "predicted for 14600000 users. Time taken : 1:30:52.496653\n",
      "predicted for 14800000 users. Time taken : 1:32:06.435484\n",
      "predicted for 15000000 users. Time taken : 1:33:19.654274\n",
      "predicted for 15200000 users. Time taken : 1:34:32.669429\n",
      "predicted for 15400000 users. Time taken : 1:35:46.186914\n",
      "predicted for 15600000 users. Time taken : 1:36:58.848625\n",
      "predicted for 15800000 users. Time taken : 1:38:12.895072\n",
      "predicted for 16000000 users. Time taken : 1:39:26.440276\n",
      "predicted for 16200000 users. Time taken : 1:40:40.023966\n",
      "predicted for 16400000 users. Time taken : 1:41:50.666453\n",
      "predicted for 16600000 users. Time taken : 1:43:03.415655\n",
      "predicted for 16800000 users. Time taken : 1:44:17.858124\n",
      "predicted for 17000000 users. Time taken : 1:45:31.773645\n",
      "predicted for 17200000 users. Time taken : 1:46:45.321815\n",
      "predicted for 17400000 users. Time taken : 1:47:58.687356\n",
      "predicted for 17600000 users. Time taken : 1:49:12.234717\n",
      "predicted for 17800000 users. Time taken : 1:50:24.752897\n",
      "predicted for 18000000 users. Time taken : 1:51:35.841691\n",
      "predicted for 18200000 users. Time taken : 1:52:49.512282\n",
      "predicted for 18400000 users. Time taken : 1:54:03.140077\n",
      "predicted for 18600000 users. Time taken : 1:55:15.710116\n",
      "predicted for 18800000 users. Time taken : 1:56:26.799382\n",
      "predicted for 19000000 users. Time taken : 1:57:37.659745\n",
      "predicted for 19200000 users. Time taken : 1:58:50.477518\n",
      "predicted for 19400000 users. Time taken : 2:00:03.978744\n",
      "predicted for 19600000 users. Time taken : 2:01:16.620526\n",
      "predicted for 19800000 users. Time taken : 2:02:29.442950\n",
      "predicted for 20000000 users. Time taken : 2:03:42.831289\n",
      "Done\n",
      "2:04:18.052778\n",
      "Cold Start cases handled :  138011\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "def predict_ratings(users, movies, averages, verbose=False):\n",
    "    cold_start_count = 0\n",
    "    predicted = list()\n",
    "    count= 0\n",
    "    print('predicting ratings for {} user_movie pairs...'.format(len(users)))\n",
    "    for user, movie in zip(users, movies):\n",
    "        # predict the rating for this (user, movie) pair\n",
    "        # ratings given by this user to similar movies\n",
    "\n",
    "        # print(user, movie)\n",
    "        try:\n",
    "            top_ratings = train_sparse_matrix[user, similar_movies[movie]].toarray().ravel()\n",
    "            top_ratings = top_ratings[top_ratings != 0][:10]\n",
    "            \n",
    "            if(len(top_ratings) != 0):\n",
    "                pred_rating = np.mean([np.mean(top_ratings), averages['global']])\n",
    "            else :       \n",
    "                # user didn't rate any of  the top 100 similar movies..\n",
    "                # so just take this movie average rating\n",
    "                pred_rating =  averages['movie'][movie]\n",
    "                \n",
    "        except KeyError:\n",
    "            # It is a new User or new Movie or there are no ratings for given user for top similar movies...\n",
    "            ########## Cold STart Problem ##########\n",
    "            cold_start_count = cold_start_count + 1\n",
    "            pred_averge = averages['global']\n",
    "            \n",
    "        except:\n",
    "            # we just want KeyErrors to be resolved. Not every Exception...\n",
    "            raise\n",
    "        \n",
    "        # add it to the prdicted ratings list..\n",
    "        predicted.append(pred_rating)\n",
    "        \n",
    "        count = count + 1\n",
    "        if verbose:\n",
    "            if count % 200000 == 0:\n",
    "                print(\"predicted for {} users. Time taken : {}\".format(count, datetime.now() - start))\n",
    "            \n",
    "    print('Done')\n",
    "    print(datetime.now() - start)\n",
    "    print(\"Cold Start cases handled : \", cold_start_count)\n",
    "    return predicted\n",
    "\n",
    "predicted_ratings = predict_ratings(users = cv_df.user.values,\n",
    "                                   movies=cv_df.movie.values,\n",
    "                                   averages=averages, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2649430x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sparse_matrix[:, 10781]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
